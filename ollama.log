nohup: ignoring input
Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAx3arUznw27DnSlyMxbGEWLgxGGhVuAJxy76Tmi7Am5

time=2024-04-02T15:10:01.746Z level=INFO source=images.go:804 msg="total blobs: 0"
time=2024-04-02T15:10:01.746Z level=INFO source=images.go:811 msg="total unused blobs removed: 0"
time=2024-04-02T15:10:01.746Z level=INFO source=routes.go:1118 msg="Listening on 127.0.0.1:11434 (version 0.1.30)"
time=2024-04-02T15:10:01.747Z level=INFO source=payload_common.go:113 msg="Extracting dynamic libraries to /tmp/ollama1664137075/runners ..."
time=2024-04-02T15:10:07.070Z level=INFO source=payload_common.go:140 msg="Dynamic LLM libraries [rocm_v60000 cpu_avx2 cuda_v11 cpu cpu_avx]"
time=2024-04-02T15:10:07.070Z level=INFO source=gpu.go:115 msg="Detecting GPU type"
time=2024-04-02T15:10:07.070Z level=INFO source=gpu.go:265 msg="Searching for GPU management library libcudart.so*"
time=2024-04-02T15:10:07.072Z level=INFO source=gpu.go:311 msg="Discovered GPU libraries: [/tmp/ollama1664137075/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.1.105]"
time=2024-04-02T15:10:07.115Z level=INFO source=gpu.go:120 msg="Nvidia GPU detected via cudart"
time=2024-04-02T15:10:07.115Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:10:07.225Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
[GIN] 2024/04/02 - 15:10:07 | 200 |      72.924µs |       127.0.0.1 | HEAD     "/"
time=2024-04-02T15:10:09.643Z level=INFO source=download.go:136 msg="downloading e8a35b5937a5 in 42 100 MB part(s)"
time=2024-04-02T15:10:31.775Z level=INFO source=download.go:136 msg="downloading 43070e2d4e53 in 1 11 KB part(s)"
time=2024-04-02T15:10:34.923Z level=INFO source=download.go:136 msg="downloading e6836092461f in 1 42 B part(s)"
time=2024-04-02T15:10:38.690Z level=INFO source=download.go:136 msg="downloading ed11eda7790d in 1 30 B part(s)"
time=2024-04-02T15:10:41.992Z level=INFO source=download.go:136 msg="downloading f9b1e3196ecf in 1 483 B part(s)"
[GIN] 2024/04/02 - 15:10:58 | 200 | 51.010332297s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/04/02 - 15:11:02 | 200 |       37.95µs |       127.0.0.1 | HEAD     "/"
time=2024-04-02T15:11:04.695Z level=INFO source=download.go:136 msg="downloading fc59fa5607aa in 39 100 MB part(s)"
time=2024-04-02T15:11:18.449Z level=INFO source=download.go:178 msg="fc59fa5607aa part 2 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2024-04-02T15:11:27.860Z level=INFO source=download.go:136 msg="downloading 6892fbdb79c4 in 1 56 B part(s)"
time=2024-04-02T15:11:31.142Z level=INFO source=download.go:136 msg="downloading fa304d675061 in 1 91 B part(s)"
time=2024-04-02T15:11:34.505Z level=INFO source=download.go:136 msg="downloading 081016dd2ef5 in 1 409 B part(s)"
[GIN] 2024/04/02 - 15:11:49 | 200 | 47.088844224s |       127.0.0.1 | POST     "/api/pull"
time=2024-04-02T15:13:17.693Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:17.694Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:17.694Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:17.694Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:17.694Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:17.705Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:13:17.705Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes
ggml_cuda_init: CUDA_USE_TENSOR_CORES: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927098480192","timestamp":1712070800}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927098480192","timestamp":1712070800}
time=2024-04-02T15:13:20.684Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070800}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070800}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":547,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070800}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070800}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     373.89 ms /   547 tokens (    0.68 ms per token,  1462.99 tokens per second)","n_prompt_tokens_processed":547,"n_tokens_second":1462.9932252982821,"slot_id":0,"t_prompt_processing":373.891,"t_token":0.6835301645338209,"task_id":0,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 41666.67 tokens per second)","n_decoded":1,"n_tokens_second":41666.666666666664,"slot_id":0,"t_token":0.024,"t_token_generation":0.024,"task_id":0,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     373.92 ms","slot_id":0,"t_prompt_processing":373.891,"t_token_generation":0.024,"t_total":373.915,"task_id":0,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070801,"truncated":false}
[GIN] 2024/04/02 - 15:13:21 | 200 |  3.980526563s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.55 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.555,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":4,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.57 ms","slot_id":0,"t_prompt_processing":12.555,"t_token_generation":0.013,"t_total":12.568,"task_id":4,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070801,"truncated":false}
[GIN] 2024/04/02 - 15:13:21 | 200 |   20.182403ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.17 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.166,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 90909.09 tokens per second)","n_decoded":1,"n_tokens_second":90909.09090909091,"slot_id":0,"t_token":0.011,"t_token_generation":0.011,"task_id":8,"tid":"139923868976704","timestamp":1712070801}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.18 ms","slot_id":0,"t_prompt_processing":12.166,"t_token_generation":0.011,"t_total":12.177,"task_id":8,"tid":"139923868976704","timestamp":1712070801}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070801,"truncated":false}
[GIN] 2024/04/02 - 15:13:21 | 200 |   19.436371ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T15:13:21.128Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T15:13:22.538Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:22.538Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:22.539Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:22.539Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:22.539Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:22.539Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:13:22.539Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928743724608","timestamp":1712070804}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928743724608","timestamp":1712070804}
time=2024-04-02T15:13:24.464Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070804}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070804}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":539,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070804}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070804}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     390.71 ms /   539 tokens (    0.72 ms per token,  1379.52 tokens per second)","n_prompt_tokens_processed":539,"n_tokens_second":1379.522158094775,"slot_id":0,"t_prompt_processing":390.715,"t_token":0.7248886827458255,"task_id":0,"tid":"139923868976704","timestamp":1712070806}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1829.99 ms /   169 runs   (   10.83 ms per token,    92.35 tokens per second)","n_decoded":169,"n_tokens_second":92.3501809571741,"slot_id":0,"t_token":10.828349112426036,"t_token_generation":1829.991,"task_id":0,"tid":"139923868976704","timestamp":1712070806}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2220.71 ms","slot_id":0,"t_prompt_processing":390.715,"t_token_generation":1829.991,"t_total":2220.706,"task_id":0,"tid":"139923868976704","timestamp":1712070806}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":708,"n_ctx":2048,"n_past":707,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070806,"truncated":false}
[GIN] 2024/04/02 - 15:13:26 | 200 |  5.560967113s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T15:13:30.424Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T15:13:30.960Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:30.960Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:30.960Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:30.960Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:30.960Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:30.961Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:13:30.961Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927375291968","timestamp":1712070812}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927375291968","timestamp":1712070812}
time=2024-04-02T15:13:32.493Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070812}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":547,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     370.86 ms /   547 tokens (    0.68 ms per token,  1474.93 tokens per second)","n_prompt_tokens_processed":547,"n_tokens_second":1474.9342076879936,"slot_id":0,"t_prompt_processing":370.864,"t_token":0.6779963436928702,"task_id":0,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 43478.26 tokens per second)","n_decoded":1,"n_tokens_second":43478.260869565216,"slot_id":0,"t_token":0.023,"t_token_generation":0.023,"task_id":0,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     370.89 ms","slot_id":0,"t_prompt_processing":370.864,"t_token_generation":0.023,"t_total":370.887,"task_id":0,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070812,"truncated":false}
[GIN] 2024/04/02 - 15:13:32 | 200 |  2.444587528s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.58 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.584,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 83333.33 tokens per second)","n_decoded":1,"n_tokens_second":83333.33333333333,"slot_id":0,"t_token":0.012,"t_token_generation":0.012,"task_id":4,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.60 ms","slot_id":0,"t_prompt_processing":12.584,"t_token_generation":0.012,"t_total":12.596,"task_id":4,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070812,"truncated":false}
[GIN] 2024/04/02 - 15:13:32 | 200 |   19.824709ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.46 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.465,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":8,"tid":"139923868976704","timestamp":1712070812}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.48 ms","slot_id":0,"t_prompt_processing":12.465,"t_token_generation":0.013,"t_total":12.478,"task_id":8,"tid":"139923868976704","timestamp":1712070812}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070812,"truncated":false}
[GIN] 2024/04/02 - 15:13:32 | 200 |   19.896684ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T15:13:32.960Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T15:13:34.299Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:34.300Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:34.300Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:34.300Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:34.300Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:34.300Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:13:34.301Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928600036928","timestamp":1712070816}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928600036928","timestamp":1712070816}
time=2024-04-02T15:13:36.038Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070816}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070816}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":539,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070816}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070816}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     392.55 ms /   539 tokens (    0.73 ms per token,  1373.07 tokens per second)","n_prompt_tokens_processed":539,"n_tokens_second":1373.0664981964171,"slot_id":0,"t_prompt_processing":392.552,"t_token":0.7282968460111318,"task_id":0,"tid":"139923868976704","timestamp":1712070818}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    2043.15 ms /   187 runs   (   10.93 ms per token,    91.53 tokens per second)","n_decoded":187,"n_tokens_second":91.52520638444601,"slot_id":0,"t_token":10.925951871657754,"t_token_generation":2043.153,"task_id":0,"tid":"139923868976704","timestamp":1712070818}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2435.70 ms","slot_id":0,"t_prompt_processing":392.552,"t_token_generation":2043.153,"t_total":2435.705,"task_id":0,"tid":"139923868976704","timestamp":1712070818}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":726,"n_ctx":2048,"n_past":725,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070818,"truncated":false}
[GIN] 2024/04/02 - 15:13:38 | 200 |  5.518435546s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T15:13:42.184Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T15:13:42.857Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:42.857Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:42.857Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:42.857Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:42.857Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:42.857Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:13:42.857Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924506396224","timestamp":1712070824}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924506396224","timestamp":1712070824}
time=2024-04-02T15:13:44.269Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070824}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070824}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":547,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070824}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070824}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     370.18 ms /   547 tokens (    0.68 ms per token,  1477.65 tokens per second)","n_prompt_tokens_processed":547,"n_tokens_second":1477.6475418914429,"slot_id":0,"t_prompt_processing":370.183,"t_token":0.6767513711151737,"task_id":0,"tid":"139923868976704","timestamp":1712070825}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     528.09 ms /    48 runs   (   11.00 ms per token,    90.89 tokens per second)","n_decoded":48,"n_tokens_second":90.89445827849683,"slot_id":0,"t_token":11.001770833333333,"t_token_generation":528.085,"task_id":0,"tid":"139923868976704","timestamp":1712070825}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     898.27 ms","slot_id":0,"t_prompt_processing":370.183,"t_token_generation":528.085,"t_total":898.268,"task_id":0,"tid":"139923868976704","timestamp":1712070825}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":595,"n_ctx":2048,"n_past":594,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070825,"truncated":false}
[GIN] 2024/04/02 - 15:13:45 | 200 |  2.988229907s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":51,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":51,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":51,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":51,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.43 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.427,"t_token":null,"task_id":51,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":51,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.44 ms","slot_id":0,"t_prompt_processing":13.427,"t_token_generation":0.014,"t_total":13.440999999999999,"task_id":51,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":51,"tid":"139923868976704","timestamp":1712070828,"truncated":false}
[GIN] 2024/04/02 - 15:13:48 | 200 |   21.261577ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":55,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":55,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":55,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":55,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.43 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.427,"t_token":null,"task_id":55,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 83333.33 tokens per second)","n_decoded":1,"n_tokens_second":83333.33333333333,"slot_id":0,"t_token":0.012,"t_token_generation":0.012,"task_id":55,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.44 ms","slot_id":0,"t_prompt_processing":12.427,"t_token_generation":0.012,"t_total":12.439,"task_id":55,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":55,"tid":"139923868976704","timestamp":1712070828,"truncated":false}
[GIN] 2024/04/02 - 15:13:48 | 200 |    19.30795ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.61 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.606,"t_token":null,"task_id":59,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":59,"tid":"139923868976704","timestamp":1712070828}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.62 ms","slot_id":0,"t_prompt_processing":12.606,"t_token_generation":0.013,"t_total":12.619,"task_id":59,"tid":"139923868976704","timestamp":1712070828}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712070828,"truncated":false}
[GIN] 2024/04/02 - 15:13:49 | 200 |   19.864619ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T15:13:49.022Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T15:13:50.471Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:50.471Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:50.472Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:50.472Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:50.472Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:50.472Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:13:50.472Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925202662976","timestamp":1712070832}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925202662976","timestamp":1712070832}
time=2024-04-02T15:13:52.061Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070832}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070832}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":539,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070832}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070832}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     392.67 ms /   539 tokens (    0.73 ms per token,  1372.67 tokens per second)","n_prompt_tokens_processed":539,"n_tokens_second":1372.6713610838754,"slot_id":0,"t_prompt_processing":392.665,"t_token":0.7285064935064935,"task_id":0,"tid":"139923868976704","timestamp":1712070834}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1942.14 ms /   169 runs   (   11.49 ms per token,    87.02 tokens per second)","n_decoded":169,"n_tokens_second":87.01723456139194,"slot_id":0,"t_token":11.491976331360947,"t_token_generation":1942.144,"task_id":0,"tid":"139923868976704","timestamp":1712070834}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2334.81 ms","slot_id":0,"t_prompt_processing":392.665,"t_token_generation":1942.144,"t_total":2334.809,"task_id":0,"tid":"139923868976704","timestamp":1712070834}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":708,"n_ctx":2048,"n_past":707,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070834,"truncated":false}
[GIN] 2024/04/02 - 15:13:54 | 200 |   5.38064251s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T15:13:58.123Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T15:13:58.836Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:58.836Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:58.836Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:58.837Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:13:58.837Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:13:58.837Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:13:58.837Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928717760064","timestamp":1712070840}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928717760064","timestamp":1712070840}
time=2024-04-02T15:14:00.451Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070840}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":547,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     463.29 ms /   547 tokens (    0.85 ms per token,  1180.68 tokens per second)","n_prompt_tokens_processed":547,"n_tokens_second":1180.6783180406353,"slot_id":0,"t_prompt_processing":463.293,"t_token":0.8469707495429616,"task_id":0,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 38461.54 tokens per second)","n_decoded":1,"n_tokens_second":38461.53846153846,"slot_id":0,"t_token":0.026,"t_token_generation":0.026,"task_id":0,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     463.32 ms","slot_id":0,"t_prompt_processing":463.293,"t_token_generation":0.026,"t_total":463.319,"task_id":0,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070840,"truncated":false}
[GIN] 2024/04/02 - 15:14:00 | 200 |  2.798316704s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.69 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.693,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 83333.33 tokens per second)","n_decoded":1,"n_tokens_second":83333.33333333333,"slot_id":0,"t_token":0.012,"t_token_generation":0.012,"task_id":4,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.71 ms","slot_id":0,"t_prompt_processing":12.693,"t_token_generation":0.012,"t_total":12.705,"task_id":4,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712070840,"truncated":false}
[GIN] 2024/04/02 - 15:14:00 | 200 |   20.217794ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":547,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":546,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.49 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.488,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":8,"tid":"139923868976704","timestamp":1712070840}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.50 ms","slot_id":0,"t_prompt_processing":12.488,"t_token_generation":0.013,"t_total":12.501,"task_id":8,"tid":"139923868976704","timestamp":1712070840}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":548,"n_ctx":2048,"n_past":547,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712070840,"truncated":false}
[GIN] 2024/04/02 - 15:14:01 | 200 |   22.817112ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T15:14:01.028Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T15:14:02.440Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:14:02.440Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:14:02.440Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:14:02.440Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T15:14:02.440Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T15:14:02.440Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T15:14:02.440Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927106872896","timestamp":1712070844}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927106872896","timestamp":1712070844}
time=2024-04-02T15:14:04.231Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712070844}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070844}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":539,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070844}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070844}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     394.56 ms /   539 tokens (    0.73 ms per token,  1366.08 tokens per second)","n_prompt_tokens_processed":539,"n_tokens_second":1366.0786699107869,"slot_id":0,"t_prompt_processing":394.56,"t_token":0.7320222634508349,"task_id":0,"tid":"139923868976704","timestamp":1712070846}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    2121.41 ms /   195 runs   (   10.88 ms per token,    91.92 tokens per second)","n_decoded":195,"n_tokens_second":91.92021325489473,"slot_id":0,"t_token":10.879000000000001,"t_token_generation":2121.405,"task_id":0,"tid":"139923868976704","timestamp":1712070846}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2515.97 ms","slot_id":0,"t_prompt_processing":394.56,"t_token_generation":2121.405,"t_total":2515.965,"task_id":0,"tid":"139923868976704","timestamp":1712070846}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":734,"n_ctx":2048,"n_past":733,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712070846,"truncated":false}
[GIN] 2024/04/02 - 15:14:06 | 200 |   5.72274327s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":198,"tid":"139923868976704","timestamp":1712070848}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":390,"slot_id":0,"task_id":198,"tid":"139923868976704","timestamp":1712070848}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":198,"tid":"139923868976704","timestamp":1712070848}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     282.73 ms /   390 tokens (    0.72 ms per token,  1379.40 tokens per second)","n_prompt_tokens_processed":390,"n_tokens_second":1379.4030368088395,"slot_id":0,"t_prompt_processing":282.731,"t_token":0.724951282051282,"task_id":198,"tid":"139923868976704","timestamp":1712070850}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1613.19 ms /   151 runs   (   10.68 ms per token,    93.60 tokens per second)","n_decoded":151,"n_tokens_second":93.60347337074165,"slot_id":0,"t_token":10.683364238410597,"t_token_generation":1613.188,"task_id":198,"tid":"139923868976704","timestamp":1712070850}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1895.92 ms","slot_id":0,"t_prompt_processing":282.731,"t_token_generation":1613.188,"t_total":1895.919,"task_id":198,"tid":"139923868976704","timestamp":1712070850}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":551,"n_ctx":2048,"n_past":550,"n_system_tokens":0,"slot_id":0,"task_id":198,"tid":"139923868976704","timestamp":1712070850,"truncated":false}
[GIN] 2024/04/02 - 15:14:10 | 200 |   1.90381981s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:54:25.769Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:25.769Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:25.769Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:25.769Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:25.769Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:25.769Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:54:25.769Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927383684672","timestamp":1712076867}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927383684672","timestamp":1712076867}
time=2024-04-02T16:54:27.407Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076867}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076867}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":532,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076867}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076867}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     369.43 ms /   532 tokens (    0.69 ms per token,  1440.04 tokens per second)","n_prompt_tokens_processed":532,"n_tokens_second":1440.0446089006666,"slot_id":0,"t_prompt_processing":369.433,"t_token":0.6944229323308271,"task_id":0,"tid":"139923868976704","timestamp":1712076868}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     721.03 ms /    65 runs   (   11.09 ms per token,    90.15 tokens per second)","n_decoded":65,"n_tokens_second":90.14906494616021,"slot_id":0,"t_token":11.092738461538461,"t_token_generation":721.028,"task_id":0,"tid":"139923868976704","timestamp":1712076868}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1090.46 ms","slot_id":0,"t_prompt_processing":369.433,"t_token_generation":721.028,"t_total":1090.461,"task_id":0,"tid":"139923868976704","timestamp":1712076868}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":597,"n_ctx":2048,"n_past":596,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076868,"truncated":false}
[GIN] 2024/04/02 - 16:54:28 | 200 |  3.355818525s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":68,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":532,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":68,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":68,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":531,"slot_id":0,"task_id":68,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.79 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.793,"t_token":null,"task_id":68,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":68,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.81 ms","slot_id":0,"t_prompt_processing":13.793,"t_token_generation":0.013,"t_total":13.806,"task_id":68,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":533,"n_ctx":2048,"n_past":532,"n_system_tokens":0,"slot_id":0,"task_id":68,"tid":"139923868976704","timestamp":1712076872,"truncated":false}
[GIN] 2024/04/02 - 16:54:32 | 200 |   23.309832ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":72,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":532,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":72,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":72,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":531,"slot_id":0,"task_id":72,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.40 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.404,"t_token":null,"task_id":72,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":72,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.42 ms","slot_id":0,"t_prompt_processing":13.404,"t_token_generation":0.013,"t_total":13.417,"task_id":72,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":533,"n_ctx":2048,"n_past":532,"n_system_tokens":0,"slot_id":0,"task_id":72,"tid":"139923868976704","timestamp":1712076872,"truncated":false}
[GIN] 2024/04/02 - 16:54:32 | 200 |   21.904221ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":76,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":532,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":76,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":76,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":531,"slot_id":0,"task_id":76,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.79 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.792,"t_token":null,"task_id":76,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":76,"tid":"139923868976704","timestamp":1712076872}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.81 ms","slot_id":0,"t_prompt_processing":13.792,"t_token_generation":0.015,"t_total":13.807,"task_id":76,"tid":"139923868976704","timestamp":1712076872}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":533,"n_ctx":2048,"n_past":532,"n_system_tokens":0,"slot_id":0,"task_id":76,"tid":"139923868976704","timestamp":1712076872,"truncated":false}
[GIN] 2024/04/02 - 16:54:32 | 200 |   22.814449ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:54:32.385Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:54:33.734Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:33.734Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:33.734Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:33.734Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:33.734Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:33.734Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:54:33.734Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924514788928","timestamp":1712076875}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924514788928","timestamp":1712076875}
time=2024-04-02T16:54:35.507Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076875}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076875}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":515,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076875}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076875}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     318.87 ms /   515 tokens (    0.62 ms per token,  1615.06 tokens per second)","n_prompt_tokens_processed":515,"n_tokens_second":1615.057985285724,"slot_id":0,"t_prompt_processing":318.874,"t_token":0.6191728155339806,"task_id":0,"tid":"139923868976704","timestamp":1712076880}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    4437.44 ms /   384 runs   (   11.56 ms per token,    86.54 tokens per second)","n_decoded":384,"n_tokens_second":86.53630333137724,"slot_id":0,"t_token":11.555843750000001,"t_token_generation":4437.444,"task_id":0,"tid":"139923868976704","timestamp":1712076880}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    4756.32 ms","slot_id":0,"t_prompt_processing":318.874,"t_token_generation":4437.444,"t_total":4756.318,"task_id":0,"tid":"139923868976704","timestamp":1712076880}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":899,"n_ctx":2048,"n_past":898,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076880,"truncated":false}
[GIN] 2024/04/02 - 16:54:40 | 200 |  7.884084844s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:54:43.944Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:54:44.343Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:44.343Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:44.343Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:44.343Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:44.343Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:44.343Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:54:44.343Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928709301824","timestamp":1712076886}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928709301824","timestamp":1712076886}
time=2024-04-02T16:54:46.019Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076886}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076886}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":532,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076886}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076886}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     370.00 ms /   532 tokens (    0.70 ms per token,  1437.84 tokens per second)","n_prompt_tokens_processed":532,"n_tokens_second":1437.8378378378377,"slot_id":0,"t_prompt_processing":370.0,"t_token":0.6954887218045113,"task_id":0,"tid":"139923868976704","timestamp":1712076887}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     924.24 ms /    87 runs   (   10.62 ms per token,    94.13 tokens per second)","n_decoded":87,"n_tokens_second":94.1315981381419,"slot_id":0,"t_token":10.623425287356323,"t_token_generation":924.238,"task_id":0,"tid":"139923868976704","timestamp":1712076887}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1294.24 ms","slot_id":0,"t_prompt_processing":370.0,"t_token_generation":924.238,"t_total":1294.238,"task_id":0,"tid":"139923868976704","timestamp":1712076887}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":619,"n_ctx":2048,"n_past":618,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076887,"truncated":false}
[GIN] 2024/04/02 - 16:54:47 | 200 |  3.373173135s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":90,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":532,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":90,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":90,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":531,"slot_id":0,"task_id":90,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.40 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.398,"t_token":null,"task_id":90,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":90,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.41 ms","slot_id":0,"t_prompt_processing":14.398,"t_token_generation":0.016,"t_total":14.414,"task_id":90,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":533,"n_ctx":2048,"n_past":532,"n_system_tokens":0,"slot_id":0,"task_id":90,"tid":"139923868976704","timestamp":1712076891,"truncated":false}
[GIN] 2024/04/02 - 16:54:51 | 200 |   23.775705ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":94,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":532,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":94,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":94,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":531,"slot_id":0,"task_id":94,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.15 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.15,"t_token":null,"task_id":94,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":94,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.16 ms","slot_id":0,"t_prompt_processing":14.15,"t_token_generation":0.014,"t_total":14.164,"task_id":94,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":533,"n_ctx":2048,"n_past":532,"n_system_tokens":0,"slot_id":0,"task_id":94,"tid":"139923868976704","timestamp":1712076891,"truncated":false}
[GIN] 2024/04/02 - 16:54:51 | 200 |   24.046957ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":98,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":532,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":98,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":98,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":531,"slot_id":0,"task_id":98,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.39 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.395,"t_token":null,"task_id":98,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":98,"tid":"139923868976704","timestamp":1712076891}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.41 ms","slot_id":0,"t_prompt_processing":14.395,"t_token_generation":0.014,"t_total":14.408999999999999,"task_id":98,"tid":"139923868976704","timestamp":1712076891}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":533,"n_ctx":2048,"n_past":532,"n_system_tokens":0,"slot_id":0,"task_id":98,"tid":"139923868976704","timestamp":1712076891,"truncated":false}
[GIN] 2024/04/02 - 16:54:51 | 200 |    22.74067ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:54:51.126Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:54:52.522Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:52.523Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:52.523Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:52.523Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:54:52.523Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:54:52.523Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:54:52.523Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928625215040","timestamp":1712076894}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928625215040","timestamp":1712076894}
time=2024-04-02T16:54:54.260Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076894}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076894}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":515,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076894}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076894}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     320.63 ms /   515 tokens (    0.62 ms per token,  1606.20 tokens per second)","n_prompt_tokens_processed":515,"n_tokens_second":1606.2027495696,"slot_id":0,"t_prompt_processing":320.632,"t_token":0.6225864077669903,"task_id":0,"tid":"139923868976704","timestamp":1712076897}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    3278.99 ms /   298 runs   (   11.00 ms per token,    90.88 tokens per second)","n_decoded":298,"n_tokens_second":90.88169886562561,"slot_id":0,"t_token":11.00331543624161,"t_token_generation":3278.988,"task_id":0,"tid":"139923868976704","timestamp":1712076897}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    3599.62 ms","slot_id":0,"t_prompt_processing":320.632,"t_token_generation":3278.988,"t_total":3599.62,"task_id":0,"tid":"139923868976704","timestamp":1712076897}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":813,"n_ctx":2048,"n_past":812,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076897,"truncated":false}
[GIN] 2024/04/02 - 16:54:57 | 200 |  6.738335678s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:55:01.575Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:55:02.290Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:02.290Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:55:02.290Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:02.290Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:55:02.290Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:02.290Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:55:02.290Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927677298240","timestamp":1712076903}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927677298240","timestamp":1712076903}
time=2024-04-02T16:55:03.887Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076903}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076903}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":532,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076903}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076903}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     373.38 ms /   532 tokens (    0.70 ms per token,  1424.83 tokens per second)","n_prompt_tokens_processed":532,"n_tokens_second":1424.8295293241702,"slot_id":0,"t_prompt_processing":373.378,"t_token":0.7018383458646617,"task_id":0,"tid":"139923868976704","timestamp":1712076904}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     415.22 ms /    40 runs   (   10.38 ms per token,    96.33 tokens per second)","n_decoded":40,"n_tokens_second":96.33354526713292,"slot_id":0,"t_token":10.3806,"t_token_generation":415.224,"task_id":0,"tid":"139923868976704","timestamp":1712076904}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     788.60 ms","slot_id":0,"t_prompt_processing":373.378,"t_token_generation":415.224,"t_total":788.602,"task_id":0,"tid":"139923868976704","timestamp":1712076904}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":572,"n_ctx":2048,"n_past":571,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076904,"truncated":false}
[GIN] 2024/04/02 - 16:55:04 | 200 |  3.106412567s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":43,"tid":"139923868976704","timestamp":1712076946}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":10,"slot_id":0,"task_id":43,"tid":"139923868976704","timestamp":1712076946}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":488,"slot_id":0,"task_id":43,"tid":"139923868976704","timestamp":1712076946}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     102.21 ms /    10 tokens (   10.22 ms per token,    97.83 tokens per second)","n_prompt_tokens_processed":10,"n_tokens_second":97.8339562095212,"slot_id":0,"t_prompt_processing":102.214,"t_token":10.2214,"task_id":43,"tid":"139923868976704","timestamp":1712076946}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     149.83 ms /    13 runs   (   11.53 ms per token,    86.77 tokens per second)","n_decoded":13,"n_tokens_second":86.76557942721368,"slot_id":0,"t_token":11.525307692307694,"t_token_generation":149.829,"task_id":43,"tid":"139923868976704","timestamp":1712076946}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     252.04 ms","slot_id":0,"t_prompt_processing":102.214,"t_token_generation":149.829,"t_total":252.043,"task_id":43,"tid":"139923868976704","timestamp":1712076946}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":511,"n_ctx":2048,"n_past":510,"n_system_tokens":0,"slot_id":0,"task_id":43,"tid":"139923868976704","timestamp":1712076946,"truncated":false}
[GIN] 2024/04/02 - 16:55:46 | 200 |  261.885649ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.27 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.271,"t_token":null,"task_id":59,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":59,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.29 ms","slot_id":0,"t_prompt_processing":14.271,"t_token_generation":0.017,"t_total":14.288,"task_id":59,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":499,"n_ctx":2048,"n_past":498,"n_system_tokens":0,"slot_id":0,"task_id":59,"tid":"139923868976704","timestamp":1712076950,"truncated":false}
[GIN] 2024/04/02 - 16:55:50 | 200 |   29.427308ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.41 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.413,"t_token":null,"task_id":63,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":63,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.43 ms","slot_id":0,"t_prompt_processing":13.413,"t_token_generation":0.013,"t_total":13.426,"task_id":63,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":499,"n_ctx":2048,"n_past":498,"n_system_tokens":0,"slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712076950,"truncated":false}
[GIN] 2024/04/02 - 16:55:50 | 200 |   22.174269ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":67,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":67,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":67,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":67,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.21 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.213,"t_token":null,"task_id":67,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":67,"tid":"139923868976704","timestamp":1712076950}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.23 ms","slot_id":0,"t_prompt_processing":14.213,"t_token_generation":0.017,"t_total":14.229999999999999,"task_id":67,"tid":"139923868976704","timestamp":1712076950}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":499,"n_ctx":2048,"n_past":498,"n_system_tokens":0,"slot_id":0,"task_id":67,"tid":"139923868976704","timestamp":1712076950,"truncated":false}
[GIN] 2024/04/02 - 16:55:50 | 200 |   22.900418ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:55:50.385Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:55:51.736Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:51.736Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:55:51.736Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:51.736Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:55:51.736Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:51.737Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:55:51.737Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927106872896","timestamp":1712076953}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927106872896","timestamp":1712076953}
time=2024-04-02T16:55:53.469Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076953}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076953}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":482,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076953}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076953}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     306.59 ms /   482 tokens (    0.64 ms per token,  1572.14 tokens per second)","n_prompt_tokens_processed":482,"n_tokens_second":1572.1372912922511,"slot_id":0,"t_prompt_processing":306.589,"t_token":0.6360767634854771,"task_id":0,"tid":"139923868976704","timestamp":1712076954}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     771.49 ms /    72 runs   (   10.72 ms per token,    93.33 tokens per second)","n_decoded":72,"n_tokens_second":93.32578085810464,"slot_id":0,"t_token":10.715152777777778,"t_token_generation":771.491,"task_id":0,"tid":"139923868976704","timestamp":1712076954}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1078.08 ms","slot_id":0,"t_prompt_processing":306.589,"t_token_generation":771.491,"t_total":1078.08,"task_id":0,"tid":"139923868976704","timestamp":1712076954}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":554,"n_ctx":2048,"n_past":553,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076954,"truncated":false}
[GIN] 2024/04/02 - 16:55:54 | 200 |  4.166937854s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:55:58.257Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:55:58.981Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:58.981Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:55:58.981Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:58.981Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:55:58.981Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:55:58.982Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:55:58.982Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928709301824","timestamp":1712076960}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928709301824","timestamp":1712076960}
time=2024-04-02T16:56:00.584Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076960}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":498,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     293.36 ms /   498 tokens (    0.59 ms per token,  1697.60 tokens per second)","n_prompt_tokens_processed":498,"n_tokens_second":1697.601881679194,"slot_id":0,"t_prompt_processing":293.355,"t_token":0.589066265060241,"task_id":0,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":0,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     293.37 ms","slot_id":0,"t_prompt_processing":293.355,"t_token_generation":0.015,"t_total":293.37,"task_id":0,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":499,"n_ctx":2048,"n_past":498,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076960,"truncated":false}
[GIN] 2024/04/02 - 16:56:00 | 200 |  2.626623342s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.34 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.34,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":4,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.35 ms","slot_id":0,"t_prompt_processing":13.34,"t_token_generation":0.013,"t_total":13.353,"task_id":4,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":499,"n_ctx":2048,"n_past":498,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712076960,"truncated":false}
[GIN] 2024/04/02 - 16:56:00 | 200 |   23.069941ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.45 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.451,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 90909.09 tokens per second)","n_decoded":1,"n_tokens_second":90909.09090909091,"slot_id":0,"t_token":0.011,"t_token_generation":0.011,"task_id":8,"tid":"139923868976704","timestamp":1712076960}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.46 ms","slot_id":0,"t_prompt_processing":12.451,"t_token_generation":0.011,"t_total":12.462,"task_id":8,"tid":"139923868976704","timestamp":1712076960}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":499,"n_ctx":2048,"n_past":498,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712076960,"truncated":false}
[GIN] 2024/04/02 - 16:56:00 | 200 |   22.164542ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:56:00.989Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:56:02.428Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:56:02.428Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:56:02.428Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:56:02.428Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:56:02.428Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:56:02.428Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:56:02.428Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925185877568","timestamp":1712076964}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925185877568","timestamp":1712076964}
time=2024-04-02T16:56:04.182Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076964}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076964}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":482,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076964}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076964}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     306.64 ms /   482 tokens (    0.64 ms per token,  1571.90 tokens per second)","n_prompt_tokens_processed":482,"n_tokens_second":1571.8963200667893,"slot_id":0,"t_prompt_processing":306.636,"t_token":0.6361742738589212,"task_id":0,"tid":"139923868976704","timestamp":1712076965}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     813.80 ms /    76 runs   (   10.71 ms per token,    93.39 tokens per second)","n_decoded":76,"n_tokens_second":93.38880956301409,"slot_id":0,"t_token":10.70792105263158,"t_token_generation":813.802,"task_id":0,"tid":"139923868976704","timestamp":1712076965}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1120.44 ms","slot_id":0,"t_prompt_processing":306.636,"t_token_generation":813.802,"t_total":1120.438,"task_id":0,"tid":"139923868976704","timestamp":1712076965}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":558,"n_ctx":2048,"n_past":557,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076965,"truncated":false}
[GIN] 2024/04/02 - 16:56:05 | 200 |  4.318088125s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:56:09.030Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:56:09.723Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:56:09.723Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:56:09.723Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:56:09.723Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:56:09.723Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:56:09.723Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:56:09.723Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927383684672","timestamp":1712076971}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927383684672","timestamp":1712076971}
time=2024-04-02T16:56:11.266Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712076971}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076971}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":498,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076971}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076971}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     291.12 ms /   498 tokens (    0.58 ms per token,  1710.65 tokens per second)","n_prompt_tokens_processed":498,"n_tokens_second":1710.652418099939,"slot_id":0,"t_prompt_processing":291.117,"t_token":0.5845722891566265,"task_id":0,"tid":"139923868976704","timestamp":1712076971}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      75.34 ms /     8 runs   (    9.42 ms per token,   106.18 tokens per second)","n_decoded":8,"n_tokens_second":106.18388394101486,"slot_id":0,"t_token":9.417625,"t_token_generation":75.341,"task_id":0,"tid":"139923868976704","timestamp":1712076971}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     366.46 ms","slot_id":0,"t_prompt_processing":291.117,"t_token_generation":75.341,"t_total":366.458,"task_id":0,"tid":"139923868976704","timestamp":1712076971}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":506,"n_ctx":2048,"n_past":505,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712076971,"truncated":false}
[GIN] 2024/04/02 - 16:56:11 | 200 |  2.608505267s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712076975}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712076975}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712076975}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712076975}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.13 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.133,"t_token":null,"task_id":11,"tid":"139923868976704","timestamp":1712076975}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      75.27 ms /     8 runs   (    9.41 ms per token,   106.28 tokens per second)","n_decoded":8,"n_tokens_second":106.28122010840684,"slot_id":0,"t_token":9.409,"t_token_generation":75.272,"task_id":11,"tid":"139923868976704","timestamp":1712076975}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      89.41 ms","slot_id":0,"t_prompt_processing":14.133,"t_token_generation":75.272,"t_total":89.405,"task_id":11,"tid":"139923868976704","timestamp":1712076975}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":506,"n_ctx":2048,"n_past":505,"n_system_tokens":0,"slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712076975,"truncated":false}
[GIN] 2024/04/02 - 16:56:15 | 200 |   99.310893ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712076979}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712076979}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712076979}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712076979}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.83 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.834,"t_token":null,"task_id":22,"tid":"139923868976704","timestamp":1712076979}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      55.15 ms /     6 runs   (    9.19 ms per token,   108.79 tokens per second)","n_decoded":6,"n_tokens_second":108.7863074301048,"slot_id":0,"t_token":9.192333333333334,"t_token_generation":55.154,"task_id":22,"tid":"139923868976704","timestamp":1712076979}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      69.99 ms","slot_id":0,"t_prompt_processing":14.834,"t_token_generation":55.154,"t_total":69.988,"task_id":22,"tid":"139923868976704","timestamp":1712076979}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":504,"n_ctx":2048,"n_past":503,"n_system_tokens":0,"slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712076979,"truncated":false}
[GIN] 2024/04/02 - 16:56:19 | 200 |    80.08964ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":31,"tid":"139923868976704","timestamp":1712076982}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":31,"tid":"139923868976704","timestamp":1712076982}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":31,"tid":"139923868976704","timestamp":1712076982}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":31,"tid":"139923868976704","timestamp":1712076982}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.76 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.758,"t_token":null,"task_id":31,"tid":"139923868976704","timestamp":1712076982}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      84.75 ms /     8 runs   (   10.59 ms per token,    94.40 tokens per second)","n_decoded":8,"n_tokens_second":94.396394057747,"slot_id":0,"t_token":10.593625,"t_token_generation":84.749,"task_id":31,"tid":"139923868976704","timestamp":1712076982}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      99.51 ms","slot_id":0,"t_prompt_processing":14.758,"t_token_generation":84.749,"t_total":99.50699999999999,"task_id":31,"tid":"139923868976704","timestamp":1712076982}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":506,"n_ctx":2048,"n_past":505,"n_system_tokens":0,"slot_id":0,"task_id":31,"tid":"139923868976704","timestamp":1712076982,"truncated":false}
[GIN] 2024/04/02 - 16:56:22 | 200 |  105.210214ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":42,"tid":"139923868976704","timestamp":1712076986}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":42,"tid":"139923868976704","timestamp":1712076986}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":42,"tid":"139923868976704","timestamp":1712076986}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":42,"tid":"139923868976704","timestamp":1712076986}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.60 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.602,"t_token":null,"task_id":42,"tid":"139923868976704","timestamp":1712076986}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.16 ms /     2 runs   (    5.58 ms per token,   179.18 tokens per second)","n_decoded":2,"n_tokens_second":179.17935853789643,"slot_id":0,"t_token":5.581,"t_token_generation":11.162,"task_id":42,"tid":"139923868976704","timestamp":1712076986}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      24.76 ms","slot_id":0,"t_prompt_processing":13.602,"t_token_generation":11.162,"t_total":24.764000000000003,"task_id":42,"tid":"139923868976704","timestamp":1712076986}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":42,"tid":"139923868976704","timestamp":1712076986,"truncated":false}
[GIN] 2024/04/02 - 16:56:26 | 200 |   34.452536ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":47,"tid":"139923868976704","timestamp":1712076990}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":47,"tid":"139923868976704","timestamp":1712076990}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":47,"tid":"139923868976704","timestamp":1712076990}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":47,"tid":"139923868976704","timestamp":1712076990}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.69 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.691,"t_token":null,"task_id":47,"tid":"139923868976704","timestamp":1712076990}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.51 ms /     2 runs   (    5.75 ms per token,   173.78 tokens per second)","n_decoded":2,"n_tokens_second":173.77704405248068,"slot_id":0,"t_token":5.7545,"t_token_generation":11.509,"task_id":47,"tid":"139923868976704","timestamp":1712076990}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      26.20 ms","slot_id":0,"t_prompt_processing":14.691,"t_token_generation":11.509,"t_total":26.200000000000003,"task_id":47,"tid":"139923868976704","timestamp":1712076990}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":47,"tid":"139923868976704","timestamp":1712076990,"truncated":false}
[GIN] 2024/04/02 - 16:56:30 | 200 |   35.772834ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":52,"tid":"139923868976704","timestamp":1712076994}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":498,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":52,"tid":"139923868976704","timestamp":1712076994}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":52,"tid":"139923868976704","timestamp":1712076994}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":497,"slot_id":0,"task_id":52,"tid":"139923868976704","timestamp":1712076994}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      15.10 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":15.104,"t_token":null,"task_id":52,"tid":"139923868976704","timestamp":1712076994}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      79.73 ms /     8 runs   (    9.97 ms per token,   100.34 tokens per second)","n_decoded":8,"n_tokens_second":100.34241850314197,"slot_id":0,"t_token":9.965875,"t_token_generation":79.727,"task_id":52,"tid":"139923868976704","timestamp":1712076994}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      94.83 ms","slot_id":0,"t_prompt_processing":15.104,"t_token_generation":79.727,"t_total":94.831,"task_id":52,"tid":"139923868976704","timestamp":1712076994}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":506,"n_ctx":2048,"n_past":505,"n_system_tokens":0,"slot_id":0,"task_id":52,"tid":"139923868976704","timestamp":1712076994,"truncated":false}
[GIN] 2024/04/02 - 16:56:34 | 200 |  104.076757ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712077042}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":494,"n_past_se":0,"n_prompt_tokens_processed":5,"slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712077042}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":494,"slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712077042}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      32.42 ms /     5 tokens (    6.48 ms per token,   154.22 tokens per second)","n_prompt_tokens_processed":5,"n_tokens_second":154.22102957959348,"slot_id":0,"t_prompt_processing":32.421,"t_token":6.4841999999999995,"task_id":63,"tid":"139923868976704","timestamp":1712077042}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      78.86 ms /     8 runs   (    9.86 ms per token,   101.45 tokens per second)","n_decoded":8,"n_tokens_second":101.45074566298064,"slot_id":0,"t_token":9.857,"t_token_generation":78.856,"task_id":63,"tid":"139923868976704","timestamp":1712077042}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     111.28 ms","slot_id":0,"t_prompt_processing":32.421,"t_token_generation":78.856,"t_total":111.27699999999999,"task_id":63,"tid":"139923868976704","timestamp":1712077042}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":507,"n_ctx":2048,"n_past":506,"n_system_tokens":0,"slot_id":0,"task_id":63,"tid":"139923868976704","timestamp":1712077042,"truncated":false}
[GIN] 2024/04/02 - 16:57:22 | 200 |  118.328613ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":74,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":74,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":74,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":74,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.46 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.455,"t_token":null,"task_id":74,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":74,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.47 ms","slot_id":0,"t_prompt_processing":13.455,"t_token_generation":0.014,"t_total":13.469,"task_id":74,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":74,"tid":"139923868976704","timestamp":1712077046,"truncated":false}
[GIN] 2024/04/02 - 16:57:26 | 200 |   18.062865ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":78,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":78,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":78,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":78,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.47 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.467,"t_token":null,"task_id":78,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 33333.33 tokens per second)","n_decoded":1,"n_tokens_second":33333.333333333336,"slot_id":0,"t_token":0.03,"t_token_generation":0.03,"task_id":78,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.50 ms","slot_id":0,"t_prompt_processing":14.467,"t_token_generation":0.03,"t_total":14.497,"task_id":78,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":78,"tid":"139923868976704","timestamp":1712077046,"truncated":false}
[GIN] 2024/04/02 - 16:57:26 | 200 |   19.060063ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":82,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":82,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":82,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":82,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      19.01 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":19.013,"t_token":null,"task_id":82,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 52631.58 tokens per second)","n_decoded":1,"n_tokens_second":52631.57894736842,"slot_id":0,"t_token":0.019,"t_token_generation":0.019,"task_id":82,"tid":"139923868976704","timestamp":1712077046}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      19.03 ms","slot_id":0,"t_prompt_processing":19.013,"t_token_generation":0.019,"t_total":19.032,"task_id":82,"tid":"139923868976704","timestamp":1712077046}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":82,"tid":"139923868976704","timestamp":1712077046,"truncated":false}
[GIN] 2024/04/02 - 16:57:26 | 200 |   27.225752ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:57:26.389Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:57:27.278Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:27.278Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:27.278Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:27.278Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:27.278Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:27.278Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:57:27.278Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924531574336","timestamp":1712077048}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924531574336","timestamp":1712077048}
time=2024-04-02T16:57:28.906Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077048}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077048}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077048}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077048}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     307.66 ms /   483 tokens (    0.64 ms per token,  1569.90 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1569.8995329305117,"slot_id":0,"t_prompt_processing":307.663,"t_token":0.6369834368530021,"task_id":0,"tid":"139923868976704","timestamp":1712077049}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     527.63 ms /    49 runs   (   10.77 ms per token,    92.87 tokens per second)","n_decoded":49,"n_tokens_second":92.86722829228539,"slot_id":0,"t_token":10.768061224489795,"t_token_generation":527.635,"task_id":0,"tid":"139923868976704","timestamp":1712077049}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     835.30 ms","slot_id":0,"t_prompt_processing":307.663,"t_token_generation":527.635,"t_total":835.298,"task_id":0,"tid":"139923868976704","timestamp":1712077049}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":532,"n_ctx":2048,"n_past":531,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077049,"truncated":false}
[GIN] 2024/04/02 - 16:57:29 | 200 |  3.358159644s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:57:33.443Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:57:33.922Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:33.922Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:33.922Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:33.922Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:33.922Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:33.922Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:57:33.922Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925294917184","timestamp":1712077055}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925294917184","timestamp":1712077055}
time=2024-04-02T16:57:35.595Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077055}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":499,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     293.19 ms /   499 tokens (    0.59 ms per token,  1701.97 tokens per second)","n_prompt_tokens_processed":499,"n_tokens_second":1701.9680070943757,"slot_id":0,"t_prompt_processing":293.19,"t_token":0.5875551102204408,"task_id":0,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 47619.05 tokens per second)","n_decoded":1,"n_tokens_second":47619.04761904762,"slot_id":0,"t_token":0.021,"t_token_generation":0.021,"task_id":0,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     293.21 ms","slot_id":0,"t_prompt_processing":293.19,"t_token_generation":0.021,"t_total":293.211,"task_id":0,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077055,"truncated":false}
[GIN] 2024/04/02 - 16:57:35 | 200 |  2.450201241s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.12 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.117,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":4,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.13 ms","slot_id":0,"t_prompt_processing":13.117,"t_token_generation":0.013,"t_total":13.13,"task_id":4,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077055,"truncated":false}
[GIN] 2024/04/02 - 16:57:35 | 200 |   21.997804ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.21 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.21,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":8,"tid":"139923868976704","timestamp":1712077055}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.22 ms","slot_id":0,"t_prompt_processing":13.21,"t_token_generation":0.014,"t_total":13.224,"task_id":8,"tid":"139923868976704","timestamp":1712077055}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077055,"truncated":false}
[GIN] 2024/04/02 - 16:57:35 | 200 |   21.410977ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:57:35.999Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:57:37.349Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:37.350Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:37.350Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:37.350Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:37.350Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:37.350Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:57:37.350Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925303309888","timestamp":1712077059}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925303309888","timestamp":1712077059}
time=2024-04-02T16:57:39.146Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077059}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077059}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077059}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077059}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     308.22 ms /   483 tokens (    0.64 ms per token,  1567.04 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1567.0421511627906,"slot_id":0,"t_prompt_processing":308.224,"t_token":0.6381449275362319,"task_id":0,"tid":"139923868976704","timestamp":1712077060}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     580.90 ms /    51 runs   (   11.39 ms per token,    87.80 tokens per second)","n_decoded":51,"n_tokens_second":87.79510344328952,"slot_id":0,"t_token":11.390156862745098,"t_token_generation":580.898,"task_id":0,"tid":"139923868976704","timestamp":1712077060}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     889.12 ms","slot_id":0,"t_prompt_processing":308.224,"t_token_generation":580.898,"t_total":889.1220000000001,"task_id":0,"tid":"139923868976704","timestamp":1712077060}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":534,"n_ctx":2048,"n_past":533,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077060,"truncated":false}
[GIN] 2024/04/02 - 16:57:40 | 200 |  4.040826627s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:57:43.764Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:57:44.395Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:44.395Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:44.395Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:44.395Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:44.395Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:44.395Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:57:44.395Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924531574336","timestamp":1712077065}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924531574336","timestamp":1712077065}
time=2024-04-02T16:57:45.875Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077065}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077065}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":499,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077065}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077065}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     289.92 ms /   499 tokens (    0.58 ms per token,  1721.15 tokens per second)","n_prompt_tokens_processed":499,"n_tokens_second":1721.152585867923,"slot_id":0,"t_prompt_processing":289.922,"t_token":0.5810060120240481,"task_id":0,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 41666.67 tokens per second)","n_decoded":1,"n_tokens_second":41666.666666666664,"slot_id":0,"t_token":0.024,"t_token_generation":0.024,"task_id":0,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     289.95 ms","slot_id":0,"t_prompt_processing":289.922,"t_token_generation":0.024,"t_total":289.946,"task_id":0,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077066,"truncated":false}
[GIN] 2024/04/02 - 16:57:46 | 200 |   2.40625853s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.83 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.826,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":4,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.84 ms","slot_id":0,"t_prompt_processing":13.826,"t_token_generation":0.014,"t_total":13.84,"task_id":4,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077066,"truncated":false}
[GIN] 2024/04/02 - 16:57:46 | 200 |    23.09217ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.97 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.967,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 47619.05 tokens per second)","n_decoded":1,"n_tokens_second":47619.04761904762,"slot_id":0,"t_token":0.021,"t_token_generation":0.021,"task_id":8,"tid":"139923868976704","timestamp":1712077066}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.99 ms","slot_id":0,"t_prompt_processing":13.967,"t_token_generation":0.021,"t_total":13.988000000000001,"task_id":8,"tid":"139923868976704","timestamp":1712077066}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077066,"truncated":false}
[GIN] 2024/04/02 - 16:57:46 | 200 |   20.384979ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:57:46.261Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:57:47.203Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:47.203Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:47.203Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:47.203Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:47.203Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:47.203Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:57:47.203Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925840180800","timestamp":1712077068}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925840180800","timestamp":1712077068}
time=2024-04-02T16:57:48.983Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077068}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077068}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077068}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077068}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     305.98 ms /   483 tokens (    0.63 ms per token,  1578.56 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1578.5603398970502,"slot_id":0,"t_prompt_processing":305.975,"t_token":0.633488612836439,"task_id":0,"tid":"139923868976704","timestamp":1712077069}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     657.93 ms /    54 runs   (   12.18 ms per token,    82.08 tokens per second)","n_decoded":54,"n_tokens_second":82.07547599976289,"slot_id":0,"t_token":12.183907407407409,"t_token_generation":657.931,"task_id":0,"tid":"139923868976704","timestamp":1712077069}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     963.91 ms","slot_id":0,"t_prompt_processing":305.975,"t_token_generation":657.931,"t_total":963.9060000000001,"task_id":0,"tid":"139923868976704","timestamp":1712077069}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":537,"n_ctx":2048,"n_past":536,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077069,"truncated":false}
[GIN] 2024/04/02 - 16:57:49 | 200 |  3.689992038s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:57:53.670Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:57:54.381Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:54.381Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:54.381Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:54.381Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:57:54.381Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:57:54.381Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:57:54.381Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925840180800","timestamp":1712077076}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925840180800","timestamp":1712077076}
time=2024-04-02T16:57:56.029Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077076}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077076}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":499,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077076}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077076}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     293.44 ms /   499 tokens (    0.59 ms per token,  1700.51 tokens per second)","n_prompt_tokens_processed":499,"n_tokens_second":1700.506403309683,"slot_id":0,"t_prompt_processing":293.442,"t_token":0.5880601202404809,"task_id":0,"tid":"139923868976704","timestamp":1712077076}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      95.78 ms /    10 runs   (    9.58 ms per token,   104.40 tokens per second)","n_decoded":10,"n_tokens_second":104.4015701996158,"slot_id":0,"t_token":9.5784,"t_token_generation":95.784,"task_id":0,"tid":"139923868976704","timestamp":1712077076}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     389.23 ms","slot_id":0,"t_prompt_processing":293.442,"t_token_generation":95.784,"t_total":389.226,"task_id":0,"tid":"139923868976704","timestamp":1712077076}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":509,"n_ctx":2048,"n_past":508,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077076,"truncated":false}
[GIN] 2024/04/02 - 16:57:56 | 200 |  2.753411796s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.34 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.342,"t_token":null,"task_id":13,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 52631.58 tokens per second)","n_decoded":1,"n_tokens_second":52631.57894736842,"slot_id":0,"t_token":0.019,"t_token_generation":0.019,"task_id":13,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.36 ms","slot_id":0,"t_prompt_processing":14.342,"t_token_generation":0.019,"t_total":14.361,"task_id":13,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712077080,"truncated":false}
[GIN] 2024/04/02 - 16:58:00 | 200 |   23.923256ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.78 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.777,"t_token":null,"task_id":17,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":17,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.79 ms","slot_id":0,"t_prompt_processing":13.777,"t_token_generation":0.015,"t_total":13.792,"task_id":17,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712077080,"truncated":false}
[GIN] 2024/04/02 - 16:58:00 | 200 |   22.210075ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.49 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.494,"t_token":null,"task_id":21,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 55555.56 tokens per second)","n_decoded":1,"n_tokens_second":55555.55555555556,"slot_id":0,"t_token":0.018,"t_token_generation":0.018,"task_id":21,"tid":"139923868976704","timestamp":1712077080}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.51 ms","slot_id":0,"t_prompt_processing":14.494,"t_token_generation":0.018,"t_total":14.512,"task_id":21,"tid":"139923868976704","timestamp":1712077080}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712077080,"truncated":false}
[GIN] 2024/04/02 - 16:58:00 | 200 |   25.941199ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:58:00.247Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:58:01.636Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:01.636Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:01.636Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:01.636Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:01.637Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:01.637Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:58:01.637Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927383684672","timestamp":1712077083}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927383684672","timestamp":1712077083}
time=2024-04-02T16:58:03.387Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077083}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077083}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077083}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077083}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     308.34 ms /   483 tokens (    0.64 ms per token,  1566.46 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1566.4627778606593,"slot_id":0,"t_prompt_processing":308.338,"t_token":0.6383809523809524,"task_id":0,"tid":"139923868976704","timestamp":1712077084}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     553.91 ms /    52 runs   (   10.65 ms per token,    93.88 tokens per second)","n_decoded":52,"n_tokens_second":93.87789735174063,"slot_id":0,"t_token":10.652134615384615,"t_token_generation":553.911,"task_id":0,"tid":"139923868976704","timestamp":1712077084}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     862.25 ms","slot_id":0,"t_prompt_processing":308.338,"t_token_generation":553.911,"t_total":862.249,"task_id":0,"tid":"139923868976704","timestamp":1712077084}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":535,"n_ctx":2048,"n_past":534,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077084,"truncated":false}
[GIN] 2024/04/02 - 16:58:04 | 200 |  4.008072627s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:58:07.985Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:58:08.692Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:08.693Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:08.693Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:08.693Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:08.693Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:08.693Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:58:08.693Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927660512832","timestamp":1712077090}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927660512832","timestamp":1712077090}
time=2024-04-02T16:58:10.281Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077090}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":499,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     293.59 ms /   499 tokens (    0.59 ms per token,  1699.66 tokens per second)","n_prompt_tokens_processed":499,"n_tokens_second":1699.6607490769377,"slot_id":0,"t_prompt_processing":293.588,"t_token":0.5883527054108217,"task_id":0,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":0,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     293.60 ms","slot_id":0,"t_prompt_processing":293.588,"t_token_generation":0.015,"t_total":293.603,"task_id":0,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077090,"truncated":false}
[GIN] 2024/04/02 - 16:58:10 | 200 |  2.595342055s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.04 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.045,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 90909.09 tokens per second)","n_decoded":1,"n_tokens_second":90909.09090909091,"slot_id":0,"t_token":0.011,"t_token_generation":0.011,"task_id":4,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.06 ms","slot_id":0,"t_prompt_processing":13.045,"t_token_generation":0.011,"t_total":13.056,"task_id":4,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077090,"truncated":false}
[GIN] 2024/04/02 - 16:58:10 | 200 |    22.54376ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.54 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.545,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":8,"tid":"139923868976704","timestamp":1712077090}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.56 ms","slot_id":0,"t_prompt_processing":12.545,"t_token_generation":0.013,"t_total":12.558,"task_id":8,"tid":"139923868976704","timestamp":1712077090}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077090,"truncated":false}
[GIN] 2024/04/02 - 16:58:10 | 200 |   21.134214ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:58:10.643Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:58:12.071Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:12.072Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:12.072Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:12.072Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:12.072Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:12.072Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:58:12.073Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924531574336","timestamp":1712077093}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924531574336","timestamp":1712077093}
time=2024-04-02T16:58:13.865Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077093}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077093}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077093}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077093}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     306.43 ms /   483 tokens (    0.63 ms per token,  1576.22 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1576.216427895441,"slot_id":0,"t_prompt_processing":306.43,"t_token":0.6344306418219462,"task_id":0,"tid":"139923868976704","timestamp":1712077094}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     771.00 ms /    72 runs   (   10.71 ms per token,    93.38 tokens per second)","n_decoded":72,"n_tokens_second":93.38460840072374,"slot_id":0,"t_token":10.708402777777778,"t_token_generation":771.005,"task_id":0,"tid":"139923868976704","timestamp":1712077094}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1077.43 ms","slot_id":0,"t_prompt_processing":306.43,"t_token_generation":771.005,"t_total":1077.435,"task_id":0,"tid":"139923868976704","timestamp":1712077094}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":555,"n_ctx":2048,"n_past":554,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077094,"truncated":false}
[GIN] 2024/04/02 - 16:58:14 | 200 |  4.304937356s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:58:18.666Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:58:19.336Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:19.336Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:19.336Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:19.336Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:19.336Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:19.337Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:58:19.337Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928600036928","timestamp":1712077100}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928600036928","timestamp":1712077100}
time=2024-04-02T16:58:20.978Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077100}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077100}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":499,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077100}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077100}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     291.67 ms /   499 tokens (    0.58 ms per token,  1710.84 tokens per second)","n_prompt_tokens_processed":499,"n_tokens_second":1710.8375904275379,"slot_id":0,"t_prompt_processing":291.67,"t_token":0.5845090180360721,"task_id":0,"tid":"139923868976704","timestamp":1712077101}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 43478.26 tokens per second)","n_decoded":1,"n_tokens_second":43478.260869565216,"slot_id":0,"t_token":0.023,"t_token_generation":0.023,"task_id":0,"tid":"139923868976704","timestamp":1712077101}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     291.69 ms","slot_id":0,"t_prompt_processing":291.67,"t_token_generation":0.023,"t_total":291.69300000000004,"task_id":0,"tid":"139923868976704","timestamp":1712077101}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077101,"truncated":false}
[GIN] 2024/04/02 - 16:58:21 | 200 |  2.609096267s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077101}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077101}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077101}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077101}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.65 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.647,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712077101}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      73.29 ms /     8 runs   (    9.16 ms per token,   109.16 tokens per second)","n_decoded":8,"n_tokens_second":109.16136779193843,"slot_id":0,"t_token":9.16075,"t_token_generation":73.286,"task_id":4,"tid":"139923868976704","timestamp":1712077101}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      86.93 ms","slot_id":0,"t_prompt_processing":13.647,"t_token_generation":73.286,"t_total":86.933,"task_id":4,"tid":"139923868976704","timestamp":1712077101}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":507,"n_ctx":2048,"n_past":506,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077101,"truncated":false}
[GIN] 2024/04/02 - 16:58:21 | 200 |  101.091307ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.64 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.641,"t_token":null,"task_id":15,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":15,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.65 ms","slot_id":0,"t_prompt_processing":13.641,"t_token_generation":0.013,"t_total":13.654,"task_id":15,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712077105,"truncated":false}
[GIN] 2024/04/02 - 16:58:25 | 200 |   18.459588ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.78 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.778,"t_token":null,"task_id":19,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 90909.09 tokens per second)","n_decoded":1,"n_tokens_second":90909.09090909091,"slot_id":0,"t_token":0.011,"t_token_generation":0.011,"task_id":19,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.79 ms","slot_id":0,"t_prompt_processing":12.778,"t_token_generation":0.011,"t_total":12.789,"task_id":19,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712077105,"truncated":false}
[GIN] 2024/04/02 - 16:58:25 | 200 |   16.787194ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.85 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.851,"t_token":null,"task_id":23,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 50000.00 tokens per second)","n_decoded":1,"n_tokens_second":50000.0,"slot_id":0,"t_token":0.02,"t_token_generation":0.02,"task_id":23,"tid":"139923868976704","timestamp":1712077105}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.87 ms","slot_id":0,"t_prompt_processing":13.851,"t_token_generation":0.02,"t_total":13.871,"task_id":23,"tid":"139923868976704","timestamp":1712077105}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712077105,"truncated":false}
[GIN] 2024/04/02 - 16:58:25 | 200 |    18.00462ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:58:25.156Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:58:25.790Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:25.790Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:25.790Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:25.790Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:25.790Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:25.790Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:58:25.790Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928709301824","timestamp":1712077107}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928709301824","timestamp":1712077107}
time=2024-04-02T16:58:27.612Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077107}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077107}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077107}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077107}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     304.38 ms /   483 tokens (    0.63 ms per token,  1586.82 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1586.816609337578,"slot_id":0,"t_prompt_processing":304.383,"t_token":0.6301925465838509,"task_id":0,"tid":"139923868976704","timestamp":1712077109}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1550.96 ms /   143 runs   (   10.85 ms per token,    92.20 tokens per second)","n_decoded":143,"n_tokens_second":92.20072161571771,"slot_id":0,"t_token":10.845902097902098,"t_token_generation":1550.964,"task_id":0,"tid":"139923868976704","timestamp":1712077109}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1855.35 ms","slot_id":0,"t_prompt_processing":304.383,"t_token_generation":1550.964,"t_total":1855.347,"task_id":0,"tid":"139923868976704","timestamp":1712077109}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":626,"n_ctx":2048,"n_past":625,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077109,"truncated":false}
[GIN] 2024/04/02 - 16:58:29 | 200 |  4.316110033s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:58:33.153Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:58:33.618Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:33.618Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:33.618Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:33.618Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:33.618Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:33.618Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:58:33.618Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924489610816","timestamp":1712077115}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924489610816","timestamp":1712077115}
time=2024-04-02T16:58:35.042Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077115}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":499,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     288.09 ms /   499 tokens (    0.58 ms per token,  1732.12 tokens per second)","n_prompt_tokens_processed":499,"n_tokens_second":1732.1216581159792,"slot_id":0,"t_prompt_processing":288.086,"t_token":0.5773266533066133,"task_id":0,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 55555.56 tokens per second)","n_decoded":1,"n_tokens_second":55555.55555555556,"slot_id":0,"t_token":0.018,"t_token_generation":0.018,"task_id":0,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     288.10 ms","slot_id":0,"t_prompt_processing":288.086,"t_token_generation":0.018,"t_total":288.104,"task_id":0,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077115,"truncated":false}
[GIN] 2024/04/02 - 16:58:35 | 200 |   2.18224112s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.92 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.923,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 83333.33 tokens per second)","n_decoded":1,"n_tokens_second":83333.33333333333,"slot_id":0,"t_token":0.012,"t_token_generation":0.012,"task_id":4,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.94 ms","slot_id":0,"t_prompt_processing":12.923,"t_token_generation":0.012,"t_total":12.935,"task_id":4,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077115,"truncated":false}
[GIN] 2024/04/02 - 16:58:35 | 200 |   22.053754ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":499,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":498,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.63 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.631,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 83333.33 tokens per second)","n_decoded":1,"n_tokens_second":83333.33333333333,"slot_id":0,"t_token":0.012,"t_token_generation":0.012,"task_id":8,"tid":"139923868976704","timestamp":1712077115}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.64 ms","slot_id":0,"t_prompt_processing":12.631,"t_token_generation":0.012,"t_total":12.643,"task_id":8,"tid":"139923868976704","timestamp":1712077115}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077115,"truncated":false}
[GIN] 2024/04/02 - 16:58:35 | 200 |   21.022347ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T16:58:35.398Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T16:58:36.727Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:36.727Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:36.728Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:36.728Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T16:58:36.728Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T16:58:36.728Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T16:58:36.728Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925294917184","timestamp":1712077118}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925294917184","timestamp":1712077118}
time=2024-04-02T16:58:38.607Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077118}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077118}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077118}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077118}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     306.20 ms /   483 tokens (    0.63 ms per token,  1577.40 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1577.3952403813182,"slot_id":0,"t_prompt_processing":306.201,"t_token":0.6339565217391305,"task_id":0,"tid":"139923868976704","timestamp":1712077119}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     535.46 ms /    48 runs   (   11.16 ms per token,    89.64 tokens per second)","n_decoded":48,"n_tokens_second":89.64321998446184,"slot_id":0,"t_token":11.155333333333333,"t_token_generation":535.456,"task_id":0,"tid":"139923868976704","timestamp":1712077119}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     841.66 ms","slot_id":0,"t_prompt_processing":306.201,"t_token_generation":535.456,"t_total":841.657,"task_id":0,"tid":"139923868976704","timestamp":1712077119}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":531,"n_ctx":2048,"n_past":530,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077119,"truncated":false}
[GIN] 2024/04/02 - 16:58:39 | 200 |  4.056063122s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:10:18.046Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:10:18.047Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:10:18.047Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:10:18.047Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:10:18.047Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:10:18.047Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:10:18.047Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928616822336","timestamp":1712077819}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928616822336","timestamp":1712077819}
time=2024-04-02T17:10:19.774Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077819}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077819}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":272,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077819}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077819}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     186.99 ms /   272 tokens (    0.69 ms per token,  1454.65 tokens per second)","n_prompt_tokens_processed":272,"n_tokens_second":1454.6465797087499,"slot_id":0,"t_prompt_processing":186.987,"t_token":0.6874522058823529,"task_id":0,"tid":"139923868976704","timestamp":1712077819}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 37037.04 tokens per second)","n_decoded":1,"n_tokens_second":37037.03703703704,"slot_id":0,"t_token":0.027,"t_token_generation":0.027,"task_id":0,"tid":"139923868976704","timestamp":1712077819}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     187.01 ms","slot_id":0,"t_prompt_processing":186.987,"t_token_generation":0.027,"t_total":187.01399999999998,"task_id":0,"tid":"139923868976704","timestamp":1712077819}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":273,"n_ctx":2048,"n_past":272,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077819,"truncated":false}
[GIN] 2024/04/02 - 17:10:19 | 200 |  2.473198351s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077819}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":272,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077819}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077819}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":271,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077819}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.41 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.408,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712077819}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":4,"tid":"139923868976704","timestamp":1712077819}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.42 ms","slot_id":0,"t_prompt_processing":12.408,"t_token_generation":0.015,"t_total":12.423,"task_id":4,"tid":"139923868976704","timestamp":1712077819}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":273,"n_ctx":2048,"n_past":272,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712077819,"truncated":false}
[GIN] 2024/04/02 - 17:10:19 | 200 |   18.989252ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077820}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":272,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077820}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077820}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":271,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077820}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.28 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.28,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712077820}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     104.14 ms /    11 runs   (    9.47 ms per token,   105.63 tokens per second)","n_decoded":11,"n_tokens_second":105.62805481135791,"slot_id":0,"t_token":9.467181818181817,"t_token_generation":104.139,"task_id":8,"tid":"139923868976704","timestamp":1712077820}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     116.42 ms","slot_id":0,"t_prompt_processing":12.28,"t_token_generation":104.139,"t_total":116.419,"task_id":8,"tid":"139923868976704","timestamp":1712077820}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":283,"n_ctx":2048,"n_past":282,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712077820,"truncated":false}
[GIN] 2024/04/02 - 17:10:20 | 200 |  122.805453ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:10:22.100Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:10:23.472Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:10:23.473Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:10:23.473Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:10:23.473Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:10:23.473Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:10:23.473Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:10:23.473Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925311702592","timestamp":1712077825}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925311702592","timestamp":1712077825}
time=2024-04-02T17:10:25.232Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077825}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077825}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":103,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077825}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077825}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     117.86 ms /   103 tokens (    1.14 ms per token,   873.94 tokens per second)","n_prompt_tokens_processed":103,"n_tokens_second":873.9404532611554,"slot_id":0,"t_prompt_processing":117.857,"t_token":1.144242718446602,"task_id":0,"tid":"139923868976704","timestamp":1712077825}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     246.04 ms /    24 runs   (   10.25 ms per token,    97.54 tokens per second)","n_decoded":24,"n_tokens_second":97.5431323538377,"slot_id":0,"t_token":10.251875,"t_token_generation":246.045,"task_id":0,"tid":"139923868976704","timestamp":1712077825}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     363.90 ms","slot_id":0,"t_prompt_processing":117.857,"t_token_generation":246.045,"t_total":363.902,"task_id":0,"tid":"139923868976704","timestamp":1712077825}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":127,"n_ctx":2048,"n_past":126,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077825,"truncated":false}
[GIN] 2024/04/02 - 17:10:25 | 200 |  3.498610813s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:13:10.308Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:13:10.982Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:13:10.982Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:13:10.982Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:13:10.982Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:13:10.982Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:13:10.983Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:13:10.983Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928625215040","timestamp":1712077992}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928625215040","timestamp":1712077992}
time=2024-04-02T17:13:12.554Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077992}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077992}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":232,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077992}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077992}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     149.68 ms /   232 tokens (    0.65 ms per token,  1549.98 tokens per second)","n_prompt_tokens_processed":232,"n_tokens_second":1549.9836316383728,"slot_id":0,"t_prompt_processing":149.679,"t_token":0.6451681034482759,"task_id":0,"tid":"139923868976704","timestamp":1712077992}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     107.62 ms /    11 runs   (    9.78 ms per token,   102.21 tokens per second)","n_decoded":11,"n_tokens_second":102.21338437807803,"slot_id":0,"t_token":9.783454545454545,"t_token_generation":107.618,"task_id":0,"tid":"139923868976704","timestamp":1712077992}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     257.30 ms","slot_id":0,"t_prompt_processing":149.679,"t_token_generation":107.618,"t_total":257.297,"task_id":0,"tid":"139923868976704","timestamp":1712077992}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":243,"n_ctx":2048,"n_past":242,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077992,"truncated":false}
[GIN] 2024/04/02 - 17:13:12 | 200 |  2.506688887s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:13:14.776Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:13:15.933Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:13:15.934Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:13:15.934Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:13:15.934Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:13:15.934Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:13:15.934Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:13:15.934Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925840180800","timestamp":1712077997}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925840180800","timestamp":1712077997}
time=2024-04-02T17:13:17.725Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712077997}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077997}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":103,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077997}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077997}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     117.52 ms /   103 tokens (    1.14 ms per token,   876.48 tokens per second)","n_prompt_tokens_processed":103,"n_tokens_second":876.4838531251329,"slot_id":0,"t_prompt_processing":117.515,"t_token":1.1409223300970874,"task_id":0,"tid":"139923868976704","timestamp":1712077998}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     390.69 ms /    37 runs   (   10.56 ms per token,    94.70 tokens per second)","n_decoded":37,"n_tokens_second":94.7049684274112,"slot_id":0,"t_token":10.55910810810811,"t_token_generation":390.687,"task_id":0,"tid":"139923868976704","timestamp":1712077998}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     508.20 ms","slot_id":0,"t_prompt_processing":117.515,"t_token_generation":390.687,"t_total":508.202,"task_id":0,"tid":"139923868976704","timestamp":1712077998}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":140,"n_ctx":2048,"n_past":139,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712077998,"truncated":false}
[GIN] 2024/04/02 - 17:13:18 | 200 |  3.460706493s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:18:11.215Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:18:11.929Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:18:11.929Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:18:11.929Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:18:11.929Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:18:11.929Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:18:11.929Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:18:11.929Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925336880704","timestamp":1712078293}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925336880704","timestamp":1712078293}
time=2024-04-02T17:18:13.567Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078293}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078293}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1136,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078293}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078293}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     691.67 ms /  1136 tokens (    0.61 ms per token,  1642.41 tokens per second)","n_prompt_tokens_processed":1136,"n_tokens_second":1642.408847031881,"slot_id":0,"t_prompt_processing":691.667,"t_token":0.6088617957746479,"task_id":0,"tid":"139923868976704","timestamp":1712078294}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      57.09 ms /     6 runs   (    9.51 ms per token,   105.10 tokens per second)","n_decoded":6,"n_tokens_second":105.09905586014818,"slot_id":0,"t_token":9.514833333333334,"t_token_generation":57.089,"task_id":0,"tid":"139923868976704","timestamp":1712078294}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     748.76 ms","slot_id":0,"t_prompt_processing":691.667,"t_token_generation":57.089,"t_total":748.7560000000001,"task_id":0,"tid":"139923868976704","timestamp":1712078294}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1142,"n_ctx":2048,"n_past":1141,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078294,"truncated":false}
[GIN] 2024/04/02 - 17:18:14 | 200 |  3.109004059s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:18:16.303Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:18:17.729Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:18:17.730Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:18:17.730Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:18:17.730Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:18:17.730Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:18:17.730Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:18:17.730Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139926511285824","timestamp":1712078299}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139926511285824","timestamp":1712078299}
time=2024-04-02T17:18:19.519Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078299}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078299}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":165,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078299}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078299}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     145.25 ms /   165 tokens (    0.88 ms per token,  1135.95 tokens per second)","n_prompt_tokens_processed":165,"n_tokens_second":1135.9489993321997,"slot_id":0,"t_prompt_processing":145.253,"t_token":0.8803212121212121,"task_id":0,"tid":"139923868976704","timestamp":1712078300}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1102.02 ms /    97 runs   (   11.36 ms per token,    88.02 tokens per second)","n_decoded":97,"n_tokens_second":88.02018112193971,"slot_id":0,"t_token":11.361030927835051,"t_token_generation":1102.02,"task_id":0,"tid":"139923868976704","timestamp":1712078300}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1247.27 ms","slot_id":0,"t_prompt_processing":145.253,"t_token_generation":1102.02,"t_total":1247.273,"task_id":0,"tid":"139923868976704","timestamp":1712078300}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":262,"n_ctx":2048,"n_past":261,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078300,"truncated":false}
[GIN] 2024/04/02 - 17:18:20 | 200 |  4.466703515s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:22:45.015Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:22:45.731Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:22:45.731Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:22:45.731Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:22:45.731Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:22:45.731Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:22:45.732Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:22:45.732Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139926511285824","timestamp":1712078567}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139926511285824","timestamp":1712078567}
time=2024-04-02T17:22:47.314Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078567}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078567}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1142,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078567}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078567}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     693.88 ms /  1142 tokens (    0.61 ms per token,  1645.81 tokens per second)","n_prompt_tokens_processed":1142,"n_tokens_second":1645.8082330764219,"slot_id":0,"t_prompt_processing":693.884,"t_token":0.6076042031523643,"task_id":0,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 38461.54 tokens per second)","n_decoded":1,"n_tokens_second":38461.53846153846,"slot_id":0,"t_token":0.026,"t_token_generation":0.026,"task_id":0,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     693.91 ms","slot_id":0,"t_prompt_processing":693.884,"t_token_generation":0.026,"t_total":693.91,"task_id":0,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1143,"n_ctx":2048,"n_past":1142,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078568,"truncated":false}
[GIN] 2024/04/02 - 17:22:48 | 200 |  3.012125293s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1142,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1141,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      15.55 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":15.553,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":4,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      15.57 ms","slot_id":0,"t_prompt_processing":15.553,"t_token_generation":0.014,"t_total":15.567,"task_id":4,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1143,"n_ctx":2048,"n_past":1142,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078568,"truncated":false}
[GIN] 2024/04/02 - 17:22:48 | 200 |   32.210551ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1142,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1141,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      15.07 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":15.074,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":8,"tid":"139923868976704","timestamp":1712078568}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      15.09 ms","slot_id":0,"t_prompt_processing":15.074,"t_token_generation":0.014,"t_total":15.088,"task_id":8,"tid":"139923868976704","timestamp":1712078568}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1143,"n_ctx":2048,"n_past":1142,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712078568,"truncated":false}
[GIN] 2024/04/02 - 17:22:48 | 200 |   28.853666ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:22:48.122Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:22:49.526Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:22:49.527Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:22:49.527Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:22:49.527Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:22:49.527Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:22:49.527Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:22:49.527Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925286524480","timestamp":1712078571}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925286524480","timestamp":1712078571}
time=2024-04-02T17:22:51.294Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078571}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078571}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1157,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078571}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078571}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     768.10 ms /  1157 tokens (    0.66 ms per token,  1506.32 tokens per second)","n_prompt_tokens_processed":1157,"n_tokens_second":1506.3201652916234,"slot_id":0,"t_prompt_processing":768.097,"t_token":0.6638694900605013,"task_id":0,"tid":"139923868976704","timestamp":1712078576}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    4925.39 ms /   434 runs   (   11.35 ms per token,    88.11 tokens per second)","n_decoded":434,"n_tokens_second":88.11486767847168,"slot_id":0,"t_token":11.348822580645162,"t_token_generation":4925.389,"task_id":0,"tid":"139923868976704","timestamp":1712078576}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    5693.49 ms","slot_id":0,"t_prompt_processing":768.097,"t_token_generation":4925.389,"t_total":5693.486,"task_id":0,"tid":"139923868976704","timestamp":1712078576}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1591,"n_ctx":2048,"n_past":1590,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078576,"truncated":false}
[GIN] 2024/04/02 - 17:22:56 | 200 |  8.874028544s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":437,"tid":"139923868976704","timestamp":1712078578}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":213,"slot_id":0,"task_id":437,"tid":"139923868976704","timestamp":1712078578}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":437,"tid":"139923868976704","timestamp":1712078578}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     161.09 ms /   213 tokens (    0.76 ms per token,  1322.28 tokens per second)","n_prompt_tokens_processed":213,"n_tokens_second":1322.2750580435295,"slot_id":0,"t_prompt_processing":161.086,"t_token":0.7562723004694837,"task_id":437,"tid":"139923868976704","timestamp":1712078580}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1606.88 ms /   151 runs   (   10.64 ms per token,    93.97 tokens per second)","n_decoded":151,"n_tokens_second":93.97110046381894,"slot_id":0,"t_token":10.641569536423841,"t_token_generation":1606.877,"task_id":437,"tid":"139923868976704","timestamp":1712078580}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1767.96 ms","slot_id":0,"t_prompt_processing":161.086,"t_token_generation":1606.877,"t_total":1767.963,"task_id":437,"tid":"139923868976704","timestamp":1712078580}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":374,"n_ctx":2048,"n_past":373,"n_system_tokens":0,"slot_id":0,"task_id":437,"tid":"139923868976704","timestamp":1712078580,"truncated":false}
[GIN] 2024/04/02 - 17:23:00 | 200 |  1.774702739s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:26:05.128Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:26:05.833Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:05.833Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:05.833Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:05.833Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:05.833Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:05.833Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:26:05.833Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925303309888","timestamp":1712078767}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925303309888","timestamp":1712078767}
time=2024-04-02T17:26:07.497Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078767}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078767}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1139,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078767}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078767}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     692.26 ms /  1139 tokens (    0.61 ms per token,  1645.33 tokens per second)","n_prompt_tokens_processed":1139,"n_tokens_second":1645.33081405595,"slot_id":0,"t_prompt_processing":692.262,"t_token":0.6077805092186128,"task_id":0,"tid":"139923868976704","timestamp":1712078768}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 43478.26 tokens per second)","n_decoded":1,"n_tokens_second":43478.260869565216,"slot_id":0,"t_token":0.023,"t_token_generation":0.023,"task_id":0,"tid":"139923868976704","timestamp":1712078768}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     692.28 ms","slot_id":0,"t_prompt_processing":692.262,"t_token_generation":0.023,"t_total":692.285,"task_id":0,"tid":"139923868976704","timestamp":1712078768}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1140,"n_ctx":2048,"n_past":1139,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078768,"truncated":false}
[GIN] 2024/04/02 - 17:26:08 | 200 |  3.067407915s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078768}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1139,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078768}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078768}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1138,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078768}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.86 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.86,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712078768}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.11 ms /     2 runs   (    5.55 ms per token,   180.08 tokens per second)","n_decoded":2,"n_tokens_second":180.08283810552854,"slot_id":0,"t_token":5.553,"t_token_generation":11.106,"task_id":4,"tid":"139923868976704","timestamp":1712078768}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      25.97 ms","slot_id":0,"t_prompt_processing":14.86,"t_token_generation":11.106,"t_total":25.966,"task_id":4,"tid":"139923868976704","timestamp":1712078768}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1141,"n_ctx":2048,"n_past":1140,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712078768,"truncated":false}
[GIN] 2024/04/02 - 17:26:08 | 200 |   34.406395ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1139,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1138,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      16.75 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":16.753,"t_token":null,"task_id":9,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":9,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      16.77 ms","slot_id":0,"t_prompt_processing":16.753,"t_token_generation":0.016,"t_total":16.769,"task_id":9,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1140,"n_ctx":2048,"n_past":1139,"n_system_tokens":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712078772,"truncated":false}
[GIN] 2024/04/02 - 17:26:12 | 200 |   32.889365ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1139,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1138,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      16.43 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":16.427,"t_token":null,"task_id":13,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":13,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      16.44 ms","slot_id":0,"t_prompt_processing":16.427,"t_token_generation":0.016,"t_total":16.442999999999998,"task_id":13,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1140,"n_ctx":2048,"n_past":1139,"n_system_tokens":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712078772,"truncated":false}
[GIN] 2024/04/02 - 17:26:12 | 200 |   36.015764ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1139,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1138,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      17.39 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":17.393,"t_token":null,"task_id":17,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":17,"tid":"139923868976704","timestamp":1712078772}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      17.41 ms","slot_id":0,"t_prompt_processing":17.393,"t_token_generation":0.016,"t_total":17.409,"task_id":17,"tid":"139923868976704","timestamp":1712078772}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1140,"n_ctx":2048,"n_past":1139,"n_system_tokens":0,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712078772,"truncated":false}
[GIN] 2024/04/02 - 17:26:12 | 200 |   32.261037ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:26:12.468Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:26:13.432Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:13.432Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:13.433Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:13.433Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:13.433Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:13.433Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:26:13.433Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928574858816","timestamp":1712078775}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928574858816","timestamp":1712078775}
time=2024-04-02T17:26:15.243Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078775}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078775}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1154,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078775}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078775}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     766.53 ms /  1154 tokens (    0.66 ms per token,  1505.49 tokens per second)","n_prompt_tokens_processed":1154,"n_tokens_second":1505.4896885697588,"slot_id":0,"t_prompt_processing":766.528,"t_token":0.6642357019064125,"task_id":0,"tid":"139923868976704","timestamp":1712078778}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    2125.30 ms /   166 runs   (   12.80 ms per token,    78.11 tokens per second)","n_decoded":166,"n_tokens_second":78.10669374365384,"slot_id":0,"t_token":12.802999999999999,"t_token_generation":2125.298,"task_id":0,"tid":"139923868976704","timestamp":1712078778}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2891.83 ms","slot_id":0,"t_prompt_processing":766.528,"t_token_generation":2125.298,"t_total":2891.826,"task_id":0,"tid":"139923868976704","timestamp":1712078778}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1320,"n_ctx":2048,"n_past":1319,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078778,"truncated":false}
[GIN] 2024/04/02 - 17:26:18 | 200 |  5.679132174s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:26:21.850Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:26:22.595Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:22.595Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:22.595Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:22.595Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:22.595Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:22.595Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:26:22.595Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925823395392","timestamp":1712078784}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925823395392","timestamp":1712078784}
time=2024-04-02T17:26:24.045Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078784}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078784}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":1139,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078784}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078784}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     693.90 ms /  1139 tokens (    0.61 ms per token,  1641.44 tokens per second)","n_prompt_tokens_processed":1139,"n_tokens_second":1641.4421633025988,"slot_id":0,"t_prompt_processing":693.902,"t_token":0.6092203687445128,"task_id":0,"tid":"139923868976704","timestamp":1712078784}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      81.30 ms /     8 runs   (   10.16 ms per token,    98.40 tokens per second)","n_decoded":8,"n_tokens_second":98.4009840098401,"slot_id":0,"t_token":10.1625,"t_token_generation":81.3,"task_id":0,"tid":"139923868976704","timestamp":1712078784}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     775.20 ms","slot_id":0,"t_prompt_processing":693.902,"t_token_generation":81.3,"t_total":775.202,"task_id":0,"tid":"139923868976704","timestamp":1712078784}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1147,"n_ctx":2048,"n_past":1146,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078784,"truncated":false}
[GIN] 2024/04/02 - 17:26:24 | 200 |  2.977983504s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712078788}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1139,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712078788}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712078788}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1138,"slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712078788}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      17.05 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":17.045,"t_token":null,"task_id":11,"tid":"139923868976704","timestamp":1712078788}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     264.82 ms /    23 runs   (   11.51 ms per token,    86.85 tokens per second)","n_decoded":23,"n_tokens_second":86.8514462653878,"slot_id":0,"t_token":11.51391304347826,"t_token_generation":264.82,"task_id":11,"tid":"139923868976704","timestamp":1712078788}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     281.87 ms","slot_id":0,"t_prompt_processing":17.045,"t_token_generation":264.82,"t_total":281.865,"task_id":11,"tid":"139923868976704","timestamp":1712078788}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1162,"n_ctx":2048,"n_past":1161,"n_system_tokens":0,"slot_id":0,"task_id":11,"tid":"139923868976704","timestamp":1712078788,"truncated":false}
[GIN] 2024/04/02 - 17:26:28 | 200 |  300.348484ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":37,"tid":"139923868976704","timestamp":1712078792}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":1139,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":37,"tid":"139923868976704","timestamp":1712078792}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":37,"tid":"139923868976704","timestamp":1712078792}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":1138,"slot_id":0,"task_id":37,"tid":"139923868976704","timestamp":1712078792}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      17.00 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":17.001,"t_token":null,"task_id":37,"tid":"139923868976704","timestamp":1712078792}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      49.12 ms /     5 runs   (    9.82 ms per token,   101.80 tokens per second)","n_decoded":5,"n_tokens_second":101.80189351521938,"slot_id":0,"t_token":9.823,"t_token_generation":49.115,"task_id":37,"tid":"139923868976704","timestamp":1712078792}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      66.12 ms","slot_id":0,"t_prompt_processing":17.001,"t_token_generation":49.115,"t_total":66.116,"task_id":37,"tid":"139923868976704","timestamp":1712078792}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":1144,"n_ctx":2048,"n_past":1143,"n_system_tokens":0,"slot_id":0,"task_id":37,"tid":"139923868976704","timestamp":1712078792,"truncated":false}
[GIN] 2024/04/02 - 17:26:32 | 200 |   83.230133ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:26:34.486Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:26:35.728Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:35.729Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:35.729Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:35.729Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:26:35.729Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:26:35.729Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:26:35.729Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925328488000","timestamp":1712078797}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925328488000","timestamp":1712078797}
time=2024-04-02T17:26:37.559Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078797}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078797}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":112,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078797}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078797}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     118.84 ms /   112 tokens (    1.06 ms per token,   942.44 tokens per second)","n_prompt_tokens_processed":112,"n_tokens_second":942.4436216762033,"slot_id":0,"t_prompt_processing":118.84,"t_token":1.0610714285714287,"task_id":0,"tid":"139923868976704","timestamp":1712078798}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     696.90 ms /    64 runs   (   10.89 ms per token,    91.83 tokens per second)","n_decoded":64,"n_tokens_second":91.83487515479199,"slot_id":0,"t_token":10.889109375,"t_token_generation":696.903,"task_id":0,"tid":"139923868976704","timestamp":1712078798}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     815.74 ms","slot_id":0,"t_prompt_processing":118.84,"t_token_generation":696.903,"t_total":815.743,"task_id":0,"tid":"139923868976704","timestamp":1712078798}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":176,"n_ctx":2048,"n_past":175,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078798,"truncated":false}
[GIN] 2024/04/02 - 17:26:38 | 200 |  3.892437039s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:27:25.100Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:27:25.493Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:25.493Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:25.493Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:25.493Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:25.493Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:25.493Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:27:25.493Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925169092160","timestamp":1712078847}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925169092160","timestamp":1712078847}
time=2024-04-02T17:27:27.240Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078847}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078847}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":404,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078847}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078847}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     274.08 ms /   404 tokens (    0.68 ms per token,  1474.02 tokens per second)","n_prompt_tokens_processed":404,"n_tokens_second":1474.0168052510023,"slot_id":0,"t_prompt_processing":274.081,"t_token":0.6784183168316832,"task_id":0,"tid":"139923868976704","timestamp":1712078847}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.31 ms /     2 runs   (    5.66 ms per token,   176.77 tokens per second)","n_decoded":2,"n_tokens_second":176.772140710624,"slot_id":0,"t_token":5.657,"t_token_generation":11.314,"task_id":0,"tid":"139923868976704","timestamp":1712078847}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     285.40 ms","slot_id":0,"t_prompt_processing":274.081,"t_token_generation":11.314,"t_total":285.39500000000004,"task_id":0,"tid":"139923868976704","timestamp":1712078847}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":406,"n_ctx":2048,"n_past":405,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078847,"truncated":false}
[GIN] 2024/04/02 - 17:27:27 | 200 |  2.429332353s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078851}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078851}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078851}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078851}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.04 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.039,"t_token":null,"task_id":5,"tid":"139923868976704","timestamp":1712078851}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     361.51 ms /    33 runs   (   10.95 ms per token,    91.28 tokens per second)","n_decoded":33,"n_tokens_second":91.28277189818374,"slot_id":0,"t_token":10.954969696969698,"t_token_generation":361.514,"task_id":5,"tid":"139923868976704","timestamp":1712078851}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     375.55 ms","slot_id":0,"t_prompt_processing":14.039,"t_token_generation":361.514,"t_total":375.553,"task_id":5,"tid":"139923868976704","timestamp":1712078851}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":437,"n_ctx":2048,"n_past":436,"n_system_tokens":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078851,"truncated":false}
[GIN] 2024/04/02 - 17:27:31 | 200 |  383.865296ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":41,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":41,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":41,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":41,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.83 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.827,"t_token":null,"task_id":41,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 47619.05 tokens per second)","n_decoded":1,"n_tokens_second":47619.04761904762,"slot_id":0,"t_token":0.021,"t_token_generation":0.021,"task_id":41,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.85 ms","slot_id":0,"t_prompt_processing":13.827,"t_token_generation":0.021,"t_total":13.848,"task_id":41,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":405,"n_ctx":2048,"n_past":404,"n_system_tokens":0,"slot_id":0,"task_id":41,"tid":"139923868976704","timestamp":1712078855,"truncated":false}
[GIN] 2024/04/02 - 17:27:35 | 200 |   21.491583ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":45,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":45,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":45,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":45,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.20 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.203,"t_token":null,"task_id":45,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 40000.00 tokens per second)","n_decoded":1,"n_tokens_second":40000.0,"slot_id":0,"t_token":0.025,"t_token_generation":0.025,"task_id":45,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.23 ms","slot_id":0,"t_prompt_processing":14.203,"t_token_generation":0.025,"t_total":14.228,"task_id":45,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":405,"n_ctx":2048,"n_past":404,"n_system_tokens":0,"slot_id":0,"task_id":45,"tid":"139923868976704","timestamp":1712078855,"truncated":false}
[GIN] 2024/04/02 - 17:27:35 | 200 |   27.535377ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":49,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":49,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":49,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":49,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      16.20 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":16.199,"t_token":null,"task_id":49,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 40000.00 tokens per second)","n_decoded":1,"n_tokens_second":40000.0,"slot_id":0,"t_token":0.025,"t_token_generation":0.025,"task_id":49,"tid":"139923868976704","timestamp":1712078855}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      16.22 ms","slot_id":0,"t_prompt_processing":16.199,"t_token_generation":0.025,"t_total":16.224,"task_id":49,"tid":"139923868976704","timestamp":1712078855}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":405,"n_ctx":2048,"n_past":404,"n_system_tokens":0,"slot_id":0,"task_id":49,"tid":"139923868976704","timestamp":1712078855,"truncated":false}
[GIN] 2024/04/02 - 17:27:35 | 200 |   21.026614ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:27:35.506Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:27:36.862Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:36.862Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:36.862Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:36.862Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:36.862Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:36.863Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:27:36.863Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928735331904","timestamp":1712078858}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928735331904","timestamp":1712078858}
time=2024-04-02T17:27:38.670Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078858}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078858}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":398,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078858}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078858}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     284.33 ms /   398 tokens (    0.71 ms per token,  1399.79 tokens per second)","n_prompt_tokens_processed":398,"n_tokens_second":1399.7917897639347,"slot_id":0,"t_prompt_processing":284.328,"t_token":0.714391959798995,"task_id":0,"tid":"139923868976704","timestamp":1712078860}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1584.40 ms /   148 runs   (   10.71 ms per token,    93.41 tokens per second)","n_decoded":148,"n_tokens_second":93.41069590337294,"slot_id":0,"t_token":10.705412162162162,"t_token_generation":1584.401,"task_id":0,"tid":"139923868976704","timestamp":1712078860}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1868.73 ms","slot_id":0,"t_prompt_processing":284.328,"t_token_generation":1584.401,"t_total":1868.729,"task_id":0,"tid":"139923868976704","timestamp":1712078860}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":546,"n_ctx":2048,"n_past":545,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078860,"truncated":false}
[GIN] 2024/04/02 - 17:27:40 | 200 |  5.039211983s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:27:44.255Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:27:44.961Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:44.961Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:44.961Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:44.961Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:44.961Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:44.961Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:27:44.961Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924506396224","timestamp":1712078866}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924506396224","timestamp":1712078866}
time=2024-04-02T17:27:46.628Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078866}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078866}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":404,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078866}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078866}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     271.05 ms /   404 tokens (    0.67 ms per token,  1490.49 tokens per second)","n_prompt_tokens_processed":404,"n_tokens_second":1490.4944088012958,"slot_id":0,"t_prompt_processing":271.051,"t_token":0.6709183168316831,"task_id":0,"tid":"139923868976704","timestamp":1712078866}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.73 ms /     2 runs   (    5.86 ms per token,   170.56 tokens per second)","n_decoded":2,"n_tokens_second":170.56114617090225,"slot_id":0,"t_token":5.863,"t_token_generation":11.726,"task_id":0,"tid":"139923868976704","timestamp":1712078866}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     282.78 ms","slot_id":0,"t_prompt_processing":271.051,"t_token_generation":11.726,"t_total":282.777,"task_id":0,"tid":"139923868976704","timestamp":1712078866}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":406,"n_ctx":2048,"n_past":405,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078866,"truncated":false}
[GIN] 2024/04/02 - 17:27:46 | 200 |  2.659926353s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078870}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078870}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078870}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078870}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.65 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.655,"t_token":null,"task_id":5,"tid":"139923868976704","timestamp":1712078870}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.34 ms /     2 runs   (    5.67 ms per token,   176.40 tokens per second)","n_decoded":2,"n_tokens_second":176.3979537837361,"slot_id":0,"t_token":5.669,"t_token_generation":11.338,"task_id":5,"tid":"139923868976704","timestamp":1712078870}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      24.99 ms","slot_id":0,"t_prompt_processing":13.655,"t_token_generation":11.338,"t_total":24.993,"task_id":5,"tid":"139923868976704","timestamp":1712078870}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":406,"n_ctx":2048,"n_past":405,"n_system_tokens":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712078870,"truncated":false}
[GIN] 2024/04/02 - 17:27:50 | 200 |   33.775326ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":10,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":10,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":10,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":10,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.49 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.492,"t_token":null,"task_id":10,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":10,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.51 ms","slot_id":0,"t_prompt_processing":13.492,"t_token_generation":0.014,"t_total":13.506,"task_id":10,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":405,"n_ctx":2048,"n_past":404,"n_system_tokens":0,"slot_id":0,"task_id":10,"tid":"139923868976704","timestamp":1712078874,"truncated":false}
[GIN] 2024/04/02 - 17:27:54 | 200 |   22.353932ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.53 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.525,"t_token":null,"task_id":14,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":14,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.54 ms","slot_id":0,"t_prompt_processing":14.525,"t_token_generation":0.015,"t_total":14.540000000000001,"task_id":14,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":405,"n_ctx":2048,"n_past":404,"n_system_tokens":0,"slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712078874,"truncated":false}
[GIN] 2024/04/02 - 17:27:54 | 200 |   25.213052ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.79 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.792,"t_token":null,"task_id":18,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 55555.56 tokens per second)","n_decoded":1,"n_tokens_second":55555.55555555556,"slot_id":0,"t_token":0.018,"t_token_generation":0.018,"task_id":18,"tid":"139923868976704","timestamp":1712078874}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.81 ms","slot_id":0,"t_prompt_processing":14.792,"t_token_generation":0.018,"t_total":14.81,"task_id":18,"tid":"139923868976704","timestamp":1712078874}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":405,"n_ctx":2048,"n_past":404,"n_system_tokens":0,"slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712078874,"truncated":false}
[GIN] 2024/04/02 - 17:27:54 | 200 |   22.783695ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:27:54.494Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:27:55.862Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:55.862Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:55.862Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:55.862Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:27:55.862Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:27:55.862Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:27:55.862Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927106872896","timestamp":1712078877}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927106872896","timestamp":1712078877}
time=2024-04-02T17:27:57.572Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078877}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078877}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":398,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078877}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078877}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     283.83 ms /   398 tokens (    0.71 ms per token,  1402.24 tokens per second)","n_prompt_tokens_processed":398,"n_tokens_second":1402.2379435722542,"slot_id":0,"t_prompt_processing":283.832,"t_token":0.713145728643216,"task_id":0,"tid":"139923868976704","timestamp":1712078879}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1324.18 ms /   120 runs   (   11.03 ms per token,    90.62 tokens per second)","n_decoded":120,"n_tokens_second":90.6221892961601,"slot_id":0,"t_token":11.034825000000001,"t_token_generation":1324.179,"task_id":0,"tid":"139923868976704","timestamp":1712078879}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1608.01 ms","slot_id":0,"t_prompt_processing":283.832,"t_token_generation":1324.179,"t_total":1608.011,"task_id":0,"tid":"139923868976704","timestamp":1712078879}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":518,"n_ctx":2048,"n_past":517,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078879,"truncated":false}
[GIN] 2024/04/02 - 17:27:59 | 200 |  4.689641953s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:28:02.872Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:28:03.364Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:28:03.364Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:28:03.364Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:28:03.364Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:28:03.364Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:28:03.364Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:28:03.364Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924523181632","timestamp":1712078884}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924523181632","timestamp":1712078884}
time=2024-04-02T17:28:04.957Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078884}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078884}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":404,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078884}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078884}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     268.76 ms /   404 tokens (    0.67 ms per token,  1503.21 tokens per second)","n_prompt_tokens_processed":404,"n_tokens_second":1503.2110672054414,"slot_id":0,"t_prompt_processing":268.758,"t_token":0.6652425742574257,"task_id":0,"tid":"139923868976704","timestamp":1712078885}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      34.64 ms /     4 runs   (    8.66 ms per token,   115.47 tokens per second)","n_decoded":4,"n_tokens_second":115.47010767587541,"slot_id":0,"t_token":8.66025,"t_token_generation":34.641,"task_id":0,"tid":"139923868976704","timestamp":1712078885}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     303.40 ms","slot_id":0,"t_prompt_processing":268.758,"t_token_generation":34.641,"t_total":303.399,"task_id":0,"tid":"139923868976704","timestamp":1712078885}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":408,"n_ctx":2048,"n_past":407,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078885,"truncated":false}
[GIN] 2024/04/02 - 17:28:05 | 200 |  2.392485591s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078885}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078885}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078885}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078885}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.45 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.448,"t_token":null,"task_id":7,"tid":"139923868976704","timestamp":1712078885}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      97.46 ms /    10 runs   (    9.75 ms per token,   102.61 tokens per second)","n_decoded":10,"n_tokens_second":102.60725022830114,"slot_id":0,"t_token":9.7459,"t_token_generation":97.459,"task_id":7,"tid":"139923868976704","timestamp":1712078885}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     110.91 ms","slot_id":0,"t_prompt_processing":13.448,"t_token_generation":97.459,"t_total":110.90700000000001,"task_id":7,"tid":"139923868976704","timestamp":1712078885}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":414,"n_ctx":2048,"n_past":413,"n_system_tokens":0,"slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078885,"truncated":false}
[GIN] 2024/04/02 - 17:28:05 | 200 |  116.981233ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712078889}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712078889}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712078889}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712078889}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.76 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.76,"t_token":null,"task_id":20,"tid":"139923868976704","timestamp":1712078889}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.23 ms /     2 runs   (    5.61 ms per token,   178.11 tokens per second)","n_decoded":2,"n_tokens_second":178.1102502449016,"slot_id":0,"t_token":5.6145,"t_token_generation":11.229,"task_id":20,"tid":"139923868976704","timestamp":1712078889}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      24.99 ms","slot_id":0,"t_prompt_processing":13.76,"t_token_generation":11.229,"t_total":24.988999999999997,"task_id":20,"tid":"139923868976704","timestamp":1712078889}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":406,"n_ctx":2048,"n_past":405,"n_system_tokens":0,"slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712078889,"truncated":false}
[GIN] 2024/04/02 - 17:28:09 | 200 |   33.710958ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712078892}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.71 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.708,"t_token":null,"task_id":25,"tid":"139923868976704","timestamp":1712078892}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      33.63 ms /     4 runs   (    8.41 ms per token,   118.94 tokens per second)","n_decoded":4,"n_tokens_second":118.94142134998512,"slot_id":0,"t_token":8.4075,"t_token_generation":33.63,"task_id":25,"tid":"139923868976704","timestamp":1712078892}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      47.34 ms","slot_id":0,"t_prompt_processing":13.708,"t_token_generation":33.63,"t_total":47.338,"task_id":25,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":408,"n_ctx":2048,"n_past":407,"n_system_tokens":0,"slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712078892,"truncated":false}
[GIN] 2024/04/02 - 17:28:12 | 200 |   55.252375ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":32,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":32,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":32,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":32,"tid":"139923868976704","timestamp":1712078892}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.66 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.657,"t_token":null,"task_id":32,"tid":"139923868976704","timestamp":1712078892}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      33.85 ms /     4 runs   (    8.46 ms per token,   118.18 tokens per second)","n_decoded":4,"n_tokens_second":118.18235537434262,"slot_id":0,"t_token":8.4615,"t_token_generation":33.846,"task_id":32,"tid":"139923868976704","timestamp":1712078892}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      47.50 ms","slot_id":0,"t_prompt_processing":13.657,"t_token_generation":33.846,"t_total":47.503,"task_id":32,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":408,"n_ctx":2048,"n_past":407,"n_system_tokens":0,"slot_id":0,"task_id":32,"tid":"139923868976704","timestamp":1712078892,"truncated":false}
[GIN] 2024/04/02 - 17:28:12 | 200 |   55.178949ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":39,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":404,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":39,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":39,"tid":"139923868976704","timestamp":1712078892}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":403,"slot_id":0,"task_id":39,"tid":"139923868976704","timestamp":1712078892}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.46 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.458,"t_token":null,"task_id":39,"tid":"139923868976704","timestamp":1712078893}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      33.55 ms /     4 runs   (    8.39 ms per token,   119.21 tokens per second)","n_decoded":4,"n_tokens_second":119.20727164357025,"slot_id":0,"t_token":8.38875,"t_token_generation":33.555,"task_id":39,"tid":"139923868976704","timestamp":1712078893}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      47.01 ms","slot_id":0,"t_prompt_processing":13.458,"t_token_generation":33.555,"t_total":47.013,"task_id":39,"tid":"139923868976704","timestamp":1712078893}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":408,"n_ctx":2048,"n_past":407,"n_system_tokens":0,"slot_id":0,"task_id":39,"tid":"139923868976704","timestamp":1712078893,"truncated":false}
[GIN] 2024/04/02 - 17:28:13 | 200 |   54.783104ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:28:13.068Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:28:14.184Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:28:14.184Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:28:14.184Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:28:14.184Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:28:14.184Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:28:14.184Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:28:14.184Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925169092160","timestamp":1712078895}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925169092160","timestamp":1712078895}
time=2024-04-02T17:28:15.768Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078895}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078895}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":398,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078895}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078895}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     281.19 ms /   398 tokens (    0.71 ms per token,  1415.44 tokens per second)","n_prompt_tokens_processed":398,"n_tokens_second":1415.438234614222,"slot_id":0,"t_prompt_processing":281.185,"t_token":0.7064949748743719,"task_id":0,"tid":"139923868976704","timestamp":1712078897}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1932.11 ms /   164 runs   (   11.78 ms per token,    84.88 tokens per second)","n_decoded":164,"n_tokens_second":84.88142737436384,"slot_id":0,"t_token":11.781140243902438,"t_token_generation":1932.107,"task_id":0,"tid":"139923868976704","timestamp":1712078897}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2213.29 ms","slot_id":0,"t_prompt_processing":281.185,"t_token_generation":1932.107,"t_total":2213.292,"task_id":0,"tid":"139923868976704","timestamp":1712078897}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":562,"n_ctx":2048,"n_past":561,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078897,"truncated":false}
[GIN] 2024/04/02 - 17:28:17 | 200 |  4.918664576s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":167,"tid":"139923868976704","timestamp":1712078899}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":119,"slot_id":0,"task_id":167,"tid":"139923868976704","timestamp":1712078899}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":167,"tid":"139923868976704","timestamp":1712078899}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     119.88 ms /   119 tokens (    1.01 ms per token,   992.64 tokens per second)","n_prompt_tokens_processed":119,"n_tokens_second":992.6427653859628,"slot_id":0,"t_prompt_processing":119.882,"t_token":1.0074117647058825,"task_id":167,"tid":"139923868976704","timestamp":1712078900}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     852.24 ms /    75 runs   (   11.36 ms per token,    88.00 tokens per second)","n_decoded":75,"n_tokens_second":88.00368911464768,"slot_id":0,"t_token":11.363159999999999,"t_token_generation":852.237,"task_id":167,"tid":"139923868976704","timestamp":1712078900}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     972.12 ms","slot_id":0,"t_prompt_processing":119.882,"t_token_generation":852.237,"t_total":972.1189999999999,"task_id":167,"tid":"139923868976704","timestamp":1712078900}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":204,"n_ctx":2048,"n_past":203,"n_system_tokens":0,"slot_id":0,"task_id":167,"tid":"139923868976704","timestamp":1712078900,"truncated":false}
[GIN] 2024/04/02 - 17:28:20 | 200 |  978.196476ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:29:02.771Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:29:03.485Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:29:03.485Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:29:03.485Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:29:03.485Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:29:03.485Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:29:03.485Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:29:03.485Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927677298240","timestamp":1712078945}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927677298240","timestamp":1712078945}
time=2024-04-02T17:29:05.117Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078945}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078945}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":456,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078945}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078945}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     288.89 ms /   456 tokens (    0.63 ms per token,  1578.44 tokens per second)","n_prompt_tokens_processed":456,"n_tokens_second":1578.4390760593024,"slot_id":0,"t_prompt_processing":288.893,"t_token":0.6335372807017543,"task_id":0,"tid":"139923868976704","timestamp":1712078945}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      32.27 ms /     4 runs   (    8.07 ms per token,   123.94 tokens per second)","n_decoded":4,"n_tokens_second":123.94261456945434,"slot_id":0,"t_token":8.06825,"t_token_generation":32.273,"task_id":0,"tid":"139923868976704","timestamp":1712078945}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     321.17 ms","slot_id":0,"t_prompt_processing":288.893,"t_token_generation":32.273,"t_total":321.166,"task_id":0,"tid":"139923868976704","timestamp":1712078945}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":460,"n_ctx":2048,"n_past":459,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078945,"truncated":false}
[GIN] 2024/04/02 - 17:29:05 | 200 |   2.67239332s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078945}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":456,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078945}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078945}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":455,"slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078945}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.92 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.92,"t_token":null,"task_id":7,"tid":"139923868976704","timestamp":1712078945}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     248.46 ms /    25 runs   (    9.94 ms per token,   100.62 tokens per second)","n_decoded":25,"n_tokens_second":100.62103301577336,"slot_id":0,"t_token":9.938279999999999,"t_token_generation":248.457,"task_id":7,"tid":"139923868976704","timestamp":1712078945}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     261.38 ms","slot_id":0,"t_prompt_processing":12.92,"t_token_generation":248.457,"t_total":261.377,"task_id":7,"tid":"139923868976704","timestamp":1712078945}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":481,"n_ctx":2048,"n_past":480,"n_system_tokens":0,"slot_id":0,"task_id":7,"tid":"139923868976704","timestamp":1712078945,"truncated":false}
[GIN] 2024/04/02 - 17:29:05 | 200 |  273.128361ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":35,"tid":"139923868976704","timestamp":1712078951}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":456,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":35,"tid":"139923868976704","timestamp":1712078951}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":35,"tid":"139923868976704","timestamp":1712078951}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":455,"slot_id":0,"task_id":35,"tid":"139923868976704","timestamp":1712078951}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.27 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.267,"t_token":null,"task_id":35,"tid":"139923868976704","timestamp":1712078951}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      95.69 ms /    10 runs   (    9.57 ms per token,   104.50 tokens per second)","n_decoded":10,"n_tokens_second":104.50303581319037,"slot_id":0,"t_token":9.5691,"t_token_generation":95.691,"task_id":35,"tid":"139923868976704","timestamp":1712078951}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     109.96 ms","slot_id":0,"t_prompt_processing":14.267,"t_token_generation":95.691,"t_total":109.958,"task_id":35,"tid":"139923868976704","timestamp":1712078951}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":466,"n_ctx":2048,"n_past":465,"n_system_tokens":0,"slot_id":0,"task_id":35,"tid":"139923868976704","timestamp":1712078951,"truncated":false}
[GIN] 2024/04/02 - 17:29:11 | 200 |   115.44834ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":48,"tid":"139923868976704","timestamp":1712078957}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":456,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":48,"tid":"139923868976704","timestamp":1712078957}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":48,"tid":"139923868976704","timestamp":1712078957}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":455,"slot_id":0,"task_id":48,"tid":"139923868976704","timestamp":1712078957}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.05 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.048,"t_token":null,"task_id":48,"tid":"139923868976704","timestamp":1712078957}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.84 ms /     2 runs   (    5.92 ms per token,   168.96 tokens per second)","n_decoded":2,"n_tokens_second":168.96173016811693,"slot_id":0,"t_token":5.9185,"t_token_generation":11.837,"task_id":48,"tid":"139923868976704","timestamp":1712078957}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      25.88 ms","slot_id":0,"t_prompt_processing":14.048,"t_token_generation":11.837,"t_total":25.884999999999998,"task_id":48,"tid":"139923868976704","timestamp":1712078957}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":458,"n_ctx":2048,"n_past":457,"n_system_tokens":0,"slot_id":0,"task_id":48,"tid":"139923868976704","timestamp":1712078957,"truncated":false}
[GIN] 2024/04/02 - 17:29:17 | 200 |    30.03867ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":53,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":456,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":53,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":53,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":455,"slot_id":0,"task_id":53,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      18.20 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":18.201,"t_token":null,"task_id":53,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 43478.26 tokens per second)","n_decoded":1,"n_tokens_second":43478.260869565216,"slot_id":0,"t_token":0.023,"t_token_generation":0.023,"task_id":53,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      18.22 ms","slot_id":0,"t_prompt_processing":18.201,"t_token_generation":0.023,"t_total":18.224,"task_id":53,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":457,"n_ctx":2048,"n_past":456,"n_system_tokens":0,"slot_id":0,"task_id":53,"tid":"139923868976704","timestamp":1712078963,"truncated":false}
[GIN] 2024/04/02 - 17:29:23 | 200 |    27.12241ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":57,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":456,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":57,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":57,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":455,"slot_id":0,"task_id":57,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.82 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.817,"t_token":null,"task_id":57,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":57,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.83 ms","slot_id":0,"t_prompt_processing":14.817,"t_token_generation":0.016,"t_total":14.833,"task_id":57,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":457,"n_ctx":2048,"n_past":456,"n_system_tokens":0,"slot_id":0,"task_id":57,"tid":"139923868976704","timestamp":1712078963,"truncated":false}
[GIN] 2024/04/02 - 17:29:23 | 200 |    24.45726ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":61,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":456,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":61,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":61,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":455,"slot_id":0,"task_id":61,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      25.88 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":25.878,"t_token":null,"task_id":61,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.04 ms /     1 runs   (    0.04 ms per token, 23255.81 tokens per second)","n_decoded":1,"n_tokens_second":23255.813953488374,"slot_id":0,"t_token":0.043,"t_token_generation":0.043,"task_id":61,"tid":"139923868976704","timestamp":1712078963}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      25.92 ms","slot_id":0,"t_prompt_processing":25.878,"t_token_generation":0.043,"t_total":25.921,"task_id":61,"tid":"139923868976704","timestamp":1712078963}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":457,"n_ctx":2048,"n_past":456,"n_system_tokens":0,"slot_id":0,"task_id":61,"tid":"139923868976704","timestamp":1712078963,"truncated":false}
[GIN] 2024/04/02 - 17:29:23 | 200 |   34.761837ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:29:23.256Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:29:24.611Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:29:24.612Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:29:24.612Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:29:24.612Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:29:24.612Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:29:24.612Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:29:24.612Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924523181632","timestamp":1712078966}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924523181632","timestamp":1712078966}
time=2024-04-02T17:29:26.466Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712078966}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078966}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":452,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078966}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078966}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     304.20 ms /   452 tokens (    0.67 ms per token,  1485.86 tokens per second)","n_prompt_tokens_processed":452,"n_tokens_second":1485.86456278764,"slot_id":0,"t_prompt_processing":304.2,"t_token":0.673008849557522,"task_id":0,"tid":"139923868976704","timestamp":1712078968}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1589.63 ms /   147 runs   (   10.81 ms per token,    92.47 tokens per second)","n_decoded":147,"n_tokens_second":92.47411668346298,"slot_id":0,"t_token":10.813836734693878,"t_token_generation":1589.634,"task_id":0,"tid":"139923868976704","timestamp":1712078968}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1893.83 ms","slot_id":0,"t_prompt_processing":304.2,"t_token_generation":1589.634,"t_total":1893.834,"task_id":0,"tid":"139923868976704","timestamp":1712078968}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":599,"n_ctx":2048,"n_past":598,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712078968,"truncated":false}
[GIN] 2024/04/02 - 17:29:28 | 200 |  5.108612346s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":150,"tid":"139923868976704","timestamp":1712078970}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":184,"slot_id":0,"task_id":150,"tid":"139923868976704","timestamp":1712078970}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":150,"tid":"139923868976704","timestamp":1712078970}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     146.76 ms /   184 tokens (    0.80 ms per token,  1253.73 tokens per second)","n_prompt_tokens_processed":184,"n_tokens_second":1253.7305297011487,"slot_id":0,"t_prompt_processing":146.762,"t_token":0.7976195652173913,"task_id":150,"tid":"139923868976704","timestamp":1712078971}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     904.98 ms /    87 runs   (   10.40 ms per token,    96.13 tokens per second)","n_decoded":87,"n_tokens_second":96.13440252468831,"slot_id":0,"t_token":10.402103448275861,"t_token_generation":904.983,"task_id":150,"tid":"139923868976704","timestamp":1712078971}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1051.74 ms","slot_id":0,"t_prompt_processing":146.762,"t_token_generation":904.983,"t_total":1051.745,"task_id":150,"tid":"139923868976704","timestamp":1712078971}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":281,"n_ctx":2048,"n_past":280,"n_system_tokens":0,"slot_id":0,"task_id":150,"tid":"139923868976704","timestamp":1712078971,"truncated":false}
[GIN] 2024/04/02 - 17:29:31 | 200 |  1.057500681s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:30:21.349Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:30:22.040Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:22.040Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:22.040Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:22.040Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:22.040Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:22.040Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:30:22.040Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928616822336","timestamp":1712079023}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928616822336","timestamp":1712079023}
time=2024-04-02T17:30:23.654Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712079023}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079023}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":458,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079023}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079023}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     287.50 ms /   458 tokens (    0.63 ms per token,  1593.02 tokens per second)","n_prompt_tokens_processed":458,"n_tokens_second":1593.0213144860593,"slot_id":0,"t_prompt_processing":287.504,"t_token":0.6277379912663756,"task_id":0,"tid":"139923868976704","timestamp":1712079023}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.23 ms /     2 runs   (    5.62 ms per token,   178.03 tokens per second)","n_decoded":2,"n_tokens_second":178.03097739006589,"slot_id":0,"t_token":5.617,"t_token_generation":11.234,"task_id":0,"tid":"139923868976704","timestamp":1712079023}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     298.74 ms","slot_id":0,"t_prompt_processing":287.504,"t_token_generation":11.234,"t_total":298.738,"task_id":0,"tid":"139923868976704","timestamp":1712079023}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":460,"n_ctx":2048,"n_past":459,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079023,"truncated":false}
[GIN] 2024/04/02 - 17:30:23 | 200 |  2.608172476s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":458,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":457,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.28 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.281,"t_token":null,"task_id":5,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 55555.56 tokens per second)","n_decoded":1,"n_tokens_second":55555.55555555556,"slot_id":0,"t_token":0.018,"t_token_generation":0.018,"task_id":5,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.30 ms","slot_id":0,"t_prompt_processing":14.281,"t_token_generation":0.018,"t_total":14.299000000000001,"task_id":5,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":459,"n_ctx":2048,"n_past":458,"n_system_tokens":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079030,"truncated":false}
[GIN] 2024/04/02 - 17:30:30 | 200 |   23.509982ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":458,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":457,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.88 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.878,"t_token":null,"task_id":9,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 52631.58 tokens per second)","n_decoded":1,"n_tokens_second":52631.57894736842,"slot_id":0,"t_token":0.019,"t_token_generation":0.019,"task_id":9,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.90 ms","slot_id":0,"t_prompt_processing":13.878,"t_token_generation":0.019,"t_total":13.897,"task_id":9,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":459,"n_ctx":2048,"n_past":458,"n_system_tokens":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079030,"truncated":false}
[GIN] 2024/04/02 - 17:30:30 | 200 |   23.658497ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":458,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":457,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      17.52 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":17.518,"t_token":null,"task_id":13,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":13,"tid":"139923868976704","timestamp":1712079030}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      17.54 ms","slot_id":0,"t_prompt_processing":17.518,"t_token_generation":0.017,"t_total":17.535,"task_id":13,"tid":"139923868976704","timestamp":1712079030}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":459,"n_ctx":2048,"n_past":458,"n_system_tokens":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079030,"truncated":false}
[GIN] 2024/04/02 - 17:30:30 | 200 |   25.765412ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:30:30.399Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:30:31.831Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:31.831Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:31.831Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:31.832Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:31.832Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:31.832Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:30:31.832Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924531574336","timestamp":1712079033}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924531574336","timestamp":1712079033}
time=2024-04-02T17:30:33.686Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712079033}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079033}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":454,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079033}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079033}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     302.84 ms /   454 tokens (    0.67 ms per token,  1499.12 tokens per second)","n_prompt_tokens_processed":454,"n_tokens_second":1499.121659996566,"slot_id":0,"t_prompt_processing":302.844,"t_token":0.667057268722467,"task_id":0,"tid":"139923868976704","timestamp":1712079036}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    2334.52 ms /   205 runs   (   11.39 ms per token,    87.81 tokens per second)","n_decoded":205,"n_tokens_second":87.81240870722144,"slot_id":0,"t_token":11.38791219512195,"t_token_generation":2334.522,"task_id":0,"tid":"139923868976704","timestamp":1712079036}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2637.37 ms","slot_id":0,"t_prompt_processing":302.844,"t_token_generation":2334.522,"t_total":2637.366,"task_id":0,"tid":"139923868976704","timestamp":1712079036}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":659,"n_ctx":2048,"n_past":658,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079036,"truncated":false}
[GIN] 2024/04/02 - 17:30:36 | 200 |  5.930755911s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:30:40.065Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:30:40.729Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:40.729Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:40.729Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:40.729Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:40.729Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:40.730Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:30:40.730Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927106872896","timestamp":1712079042}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927106872896","timestamp":1712079042}
time=2024-04-02T17:30:42.341Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712079042}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079042}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":458,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079042}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079042}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     288.30 ms /   458 tokens (    0.63 ms per token,  1588.63 tokens per second)","n_prompt_tokens_processed":458,"n_tokens_second":1588.6339828927014,"slot_id":0,"t_prompt_processing":288.298,"t_token":0.629471615720524,"task_id":0,"tid":"139923868976704","timestamp":1712079042}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.44 ms /     2 runs   (    5.72 ms per token,   174.83 tokens per second)","n_decoded":2,"n_tokens_second":174.82517482517483,"slot_id":0,"t_token":5.72,"t_token_generation":11.44,"task_id":0,"tid":"139923868976704","timestamp":1712079042}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     299.74 ms","slot_id":0,"t_prompt_processing":288.298,"t_token_generation":11.44,"t_total":299.738,"task_id":0,"tid":"139923868976704","timestamp":1712079042}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":460,"n_ctx":2048,"n_past":459,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079042,"truncated":false}
[GIN] 2024/04/02 - 17:30:42 | 200 |  2.580439277s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":458,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":457,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.85 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.851,"t_token":null,"task_id":5,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":5,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.87 ms","slot_id":0,"t_prompt_processing":13.851,"t_token_generation":0.014,"t_total":13.865,"task_id":5,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":459,"n_ctx":2048,"n_past":458,"n_system_tokens":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712079048,"truncated":false}
[GIN] 2024/04/02 - 17:30:48 | 200 |   27.826503ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":458,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":457,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.68 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.679,"t_token":null,"task_id":9,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":9,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.69 ms","slot_id":0,"t_prompt_processing":14.679,"t_token_generation":0.015,"t_total":14.694,"task_id":9,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":459,"n_ctx":2048,"n_past":458,"n_system_tokens":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712079048,"truncated":false}
[GIN] 2024/04/02 - 17:30:48 | 200 |   24.787199ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":458,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":457,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.38 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.383,"t_token":null,"task_id":13,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":13,"tid":"139923868976704","timestamp":1712079048}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.40 ms","slot_id":0,"t_prompt_processing":14.383,"t_token_generation":0.016,"t_total":14.399,"task_id":13,"tid":"139923868976704","timestamp":1712079048}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":459,"n_ctx":2048,"n_past":458,"n_system_tokens":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712079048,"truncated":false}
[GIN] 2024/04/02 - 17:30:48 | 200 |   28.701952ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:30:48.363Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:30:49.819Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:49.819Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:49.819Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:49.819Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:30:49.819Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:30:49.819Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:30:49.819Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927383684672","timestamp":1712079051}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927383684672","timestamp":1712079051}
time=2024-04-02T17:30:51.620Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712079051}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079051}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":454,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079051}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079051}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     303.30 ms /   454 tokens (    0.67 ms per token,  1496.88 tokens per second)","n_prompt_tokens_processed":454,"n_tokens_second":1496.8776582766784,"slot_id":0,"t_prompt_processing":303.298,"t_token":0.668057268722467,"task_id":0,"tid":"139923868976704","timestamp":1712079053}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1520.89 ms /   136 runs   (   11.18 ms per token,    89.42 tokens per second)","n_decoded":136,"n_tokens_second":89.42132567115307,"slot_id":0,"t_token":11.183014705882353,"t_token_generation":1520.89,"task_id":0,"tid":"139923868976704","timestamp":1712079053}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1824.19 ms","slot_id":0,"t_prompt_processing":303.298,"t_token_generation":1520.89,"t_total":1824.188,"task_id":0,"tid":"139923868976704","timestamp":1712079053}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":590,"n_ctx":2048,"n_past":589,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079053,"truncated":false}
[GIN] 2024/04/02 - 17:30:53 | 200 |  5.088329066s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":139,"tid":"139923868976704","timestamp":1712079055}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":142,"slot_id":0,"task_id":139,"tid":"139923868976704","timestamp":1712079055}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":139,"tid":"139923868976704","timestamp":1712079055}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     142.70 ms /   142 tokens (    1.00 ms per token,   995.12 tokens per second)","n_prompt_tokens_processed":142,"n_tokens_second":995.1224981779447,"slot_id":0,"t_prompt_processing":142.696,"t_token":1.0049014084507042,"task_id":139,"tid":"139923868976704","timestamp":1712079056}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     992.77 ms /    88 runs   (   11.28 ms per token,    88.64 tokens per second)","n_decoded":88,"n_tokens_second":88.64123066261334,"slot_id":0,"t_token":11.281431818181817,"t_token_generation":992.766,"task_id":139,"tid":"139923868976704","timestamp":1712079056}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1135.46 ms","slot_id":0,"t_prompt_processing":142.696,"t_token_generation":992.766,"t_total":1135.462,"task_id":139,"tid":"139923868976704","timestamp":1712079056}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":240,"n_ctx":2048,"n_past":239,"n_system_tokens":0,"slot_id":0,"task_id":139,"tid":"139923868976704","timestamp":1712079056,"truncated":false}
[GIN] 2024/04/02 - 17:30:56 | 200 |   1.14248189s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:38:41.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:38:41.521Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:38:41.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:38:41.521Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:38:41.521Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:38:41.521Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:38:41.521Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139923910809152","timestamp":1712079523}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139923910809152","timestamp":1712079523}
time=2024-04-02T17:38:43.233Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712079523}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079523}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079523}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079523}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     303.38 ms /   488 tokens (    0.62 ms per token,  1608.54 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1608.5437405234359,"slot_id":0,"t_prompt_processing":303.38,"t_token":0.6216803278688524,"task_id":0,"tid":"139923868976704","timestamp":1712079523}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     251.97 ms /    20 runs   (   12.60 ms per token,    79.37 tokens per second)","n_decoded":20,"n_tokens_second":79.37326867057712,"slot_id":0,"t_token":12.5987,"t_token_generation":251.974,"task_id":0,"tid":"139923868976704","timestamp":1712079523}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     555.35 ms","slot_id":0,"t_prompt_processing":303.38,"t_token_generation":251.974,"t_total":555.354,"task_id":0,"tid":"139923868976704","timestamp":1712079523}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":508,"n_ctx":2048,"n_past":507,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079523,"truncated":false}
[GIN] 2024/04/02 - 17:38:43 | 200 |  2.628718081s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712079527}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712079527}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712079527}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712079527}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      15.30 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":15.302,"t_token":null,"task_id":23,"tid":"139923868976704","timestamp":1712079527}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     199.62 ms /    18 runs   (   11.09 ms per token,    90.17 tokens per second)","n_decoded":18,"n_tokens_second":90.16951869514689,"slot_id":0,"t_token":11.090222222222222,"t_token_generation":199.624,"task_id":23,"tid":"139923868976704","timestamp":1712079527}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     214.93 ms","slot_id":0,"t_prompt_processing":15.302,"t_token_generation":199.624,"t_total":214.926,"task_id":23,"tid":"139923868976704","timestamp":1712079527}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":506,"n_ctx":2048,"n_past":505,"n_system_tokens":0,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712079527,"truncated":false}
[GIN] 2024/04/02 - 17:38:47 | 200 |   225.37173ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":44,"tid":"139923868976704","timestamp":1712079531}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":44,"tid":"139923868976704","timestamp":1712079531}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":44,"tid":"139923868976704","timestamp":1712079531}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":44,"tid":"139923868976704","timestamp":1712079531}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      17.08 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":17.084,"t_token":null,"task_id":44,"tid":"139923868976704","timestamp":1712079531}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     254.68 ms /    22 runs   (   11.58 ms per token,    86.38 tokens per second)","n_decoded":22,"n_tokens_second":86.38325107291924,"slot_id":0,"t_token":11.576318181818182,"t_token_generation":254.679,"task_id":44,"tid":"139923868976704","timestamp":1712079531}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     271.76 ms","slot_id":0,"t_prompt_processing":17.084,"t_token_generation":254.679,"t_total":271.763,"task_id":44,"tid":"139923868976704","timestamp":1712079531}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":510,"n_ctx":2048,"n_past":509,"n_system_tokens":0,"slot_id":0,"task_id":44,"tid":"139923868976704","timestamp":1712079531,"truncated":false}
[GIN] 2024/04/02 - 17:38:51 | 200 |  281.170885ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":69,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":69,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":69,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":69,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.42 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.423,"t_token":null,"task_id":69,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 52631.58 tokens per second)","n_decoded":1,"n_tokens_second":52631.57894736842,"slot_id":0,"t_token":0.019,"t_token_generation":0.019,"task_id":69,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.44 ms","slot_id":0,"t_prompt_processing":14.423,"t_token_generation":0.019,"t_total":14.442,"task_id":69,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":69,"tid":"139923868976704","timestamp":1712079535,"truncated":false}
[GIN] 2024/04/02 - 17:38:55 | 200 |   28.037197ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":73,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":73,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":73,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":73,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.71 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.709,"t_token":null,"task_id":73,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 41666.67 tokens per second)","n_decoded":1,"n_tokens_second":41666.666666666664,"slot_id":0,"t_token":0.024,"t_token_generation":0.024,"task_id":73,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.73 ms","slot_id":0,"t_prompt_processing":14.709,"t_token_generation":0.024,"t_total":14.732999999999999,"task_id":73,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":73,"tid":"139923868976704","timestamp":1712079535,"truncated":false}
[GIN] 2024/04/02 - 17:38:55 | 200 |   23.535439ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":77,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":77,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":77,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":77,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      16.62 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":16.618,"t_token":null,"task_id":77,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.04 ms /     1 runs   (    0.04 ms per token, 25641.03 tokens per second)","n_decoded":1,"n_tokens_second":25641.02564102564,"slot_id":0,"t_token":0.039,"t_token_generation":0.039,"task_id":77,"tid":"139923868976704","timestamp":1712079535}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      16.66 ms","slot_id":0,"t_prompt_processing":16.618,"t_token_generation":0.039,"t_total":16.657,"task_id":77,"tid":"139923868976704","timestamp":1712079535}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":77,"tid":"139923868976704","timestamp":1712079535,"truncated":false}
[GIN] 2024/04/02 - 17:38:55 | 200 |   30.317997ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:38:55.519Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:38:56.779Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:38:56.779Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:38:56.779Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:38:56.780Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:38:56.780Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:38:56.780Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:38:56.780Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924523181632","timestamp":1712079538}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924523181632","timestamp":1712079538}
time=2024-04-02T17:38:58.555Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712079538}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079538}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079538}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079538}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     304.77 ms /   484 tokens (    0.63 ms per token,  1588.08 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1588.0828165501855,"slot_id":0,"t_prompt_processing":304.77,"t_token":0.6296900826446281,"task_id":0,"tid":"139923868976704","timestamp":1712079540}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1563.96 ms /   141 runs   (   11.09 ms per token,    90.16 tokens per second)","n_decoded":141,"n_tokens_second":90.15570081351133,"slot_id":0,"t_token":11.091921985815603,"t_token_generation":1563.961,"task_id":0,"tid":"139923868976704","timestamp":1712079540}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1868.73 ms","slot_id":0,"t_prompt_processing":304.77,"t_token_generation":1563.961,"t_total":1868.731,"task_id":0,"tid":"139923868976704","timestamp":1712079540}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":625,"n_ctx":2048,"n_past":624,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712079540,"truncated":false}
[GIN] 2024/04/02 - 17:39:00 | 200 |  4.913762839s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":144,"tid":"139923868976704","timestamp":1712079542}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":144,"slot_id":0,"task_id":144,"tid":"139923868976704","timestamp":1712079542}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":144,"tid":"139923868976704","timestamp":1712079542}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     142.78 ms /   144 tokens (    0.99 ms per token,  1008.53 tokens per second)","n_prompt_tokens_processed":144,"n_tokens_second":1008.530487036181,"slot_id":0,"t_prompt_processing":142.782,"t_token":0.9915416666666668,"task_id":144,"tid":"139923868976704","timestamp":1712079543}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1346.02 ms /   126 runs   (   10.68 ms per token,    93.61 tokens per second)","n_decoded":126,"n_tokens_second":93.60944653043272,"slot_id":0,"t_token":10.68268253968254,"t_token_generation":1346.018,"task_id":144,"tid":"139923868976704","timestamp":1712079543}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1488.80 ms","slot_id":0,"t_prompt_processing":142.782,"t_token_generation":1346.018,"t_total":1488.8,"task_id":144,"tid":"139923868976704","timestamp":1712079543}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":280,"n_ctx":2048,"n_past":279,"n_system_tokens":0,"slot_id":0,"task_id":144,"tid":"139923868976704","timestamp":1712079543,"truncated":false}
[GIN] 2024/04/02 - 17:39:03 | 200 |  1.493833723s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:49:11.004Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:11.005Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:11.005Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:11.005Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:11.005Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:11.005Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:49:11.005Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927668905536","timestamp":1712080152}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927668905536","timestamp":1712080152}
time=2024-04-02T17:49:12.447Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080152}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     284.90 ms /   488 tokens (    0.58 ms per token,  1712.85 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1712.8516523051546,"slot_id":0,"t_prompt_processing":284.905,"t_token":0.5838217213114754,"task_id":0,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.04 ms /     1 runs   (    0.04 ms per token, 27777.78 tokens per second)","n_decoded":1,"n_tokens_second":27777.77777777778,"slot_id":0,"t_token":0.036,"t_token_generation":0.036,"task_id":0,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     284.94 ms","slot_id":0,"t_prompt_processing":284.905,"t_token_generation":0.036,"t_total":284.941,"task_id":0,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080152,"truncated":false}
[GIN] 2024/04/02 - 17:49:12 | 200 |  2.336897493s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      17.65 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":17.653,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 55555.56 tokens per second)","n_decoded":1,"n_tokens_second":55555.55555555556,"slot_id":0,"t_token":0.018,"t_token_generation":0.018,"task_id":4,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      17.67 ms","slot_id":0,"t_prompt_processing":17.653,"t_token_generation":0.018,"t_total":17.671,"task_id":4,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080152,"truncated":false}
[GIN] 2024/04/02 - 17:49:12 | 200 |   27.377536ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      15.93 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":15.927,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 47619.05 tokens per second)","n_decoded":1,"n_tokens_second":47619.04761904762,"slot_id":0,"t_token":0.021,"t_token_generation":0.021,"task_id":8,"tid":"139923868976704","timestamp":1712080152}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      15.95 ms","slot_id":0,"t_prompt_processing":15.927,"t_token_generation":0.021,"t_total":15.948,"task_id":8,"tid":"139923868976704","timestamp":1712080152}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080152,"truncated":false}
[GIN] 2024/04/02 - 17:49:12 | 200 |    25.13324ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:49:12.821Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:49:14.233Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:14.233Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:14.233Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:14.233Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:14.233Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:14.233Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:49:14.233Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139923927594560","timestamp":1712080156}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139923927594560","timestamp":1712080156}
time=2024-04-02T17:49:16.009Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080156}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080156}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080156}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080156}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     305.89 ms /   484 tokens (    0.63 ms per token,  1582.28 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1582.2784810126584,"slot_id":0,"t_prompt_processing":305.888,"t_token":0.632,"task_id":0,"tid":"139923868976704","timestamp":1712080157}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1637.06 ms /   148 runs   (   11.06 ms per token,    90.41 tokens per second)","n_decoded":148,"n_tokens_second":90.40619257985065,"slot_id":0,"t_token":11.06118918918919,"t_token_generation":1637.056,"task_id":0,"tid":"139923868976704","timestamp":1712080157}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1942.94 ms","slot_id":0,"t_prompt_processing":305.888,"t_token_generation":1637.056,"t_total":1942.944,"task_id":0,"tid":"139923868976704","timestamp":1712080157}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":632,"n_ctx":2048,"n_past":631,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080157,"truncated":false}
[GIN] 2024/04/02 - 17:49:17 | 200 |  5.136546154s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:49:22.544Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:49:23.302Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:23.302Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:23.302Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:23.302Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:23.302Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:23.302Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:49:23.302Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925185877568","timestamp":1712080165}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925185877568","timestamp":1712080165}
time=2024-04-02T17:49:25.006Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080165}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     290.68 ms /   488 tokens (    0.60 ms per token,  1678.82 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1678.8162969027906,"slot_id":0,"t_prompt_processing":290.681,"t_token":0.5956577868852458,"task_id":0,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 32258.06 tokens per second)","n_decoded":1,"n_tokens_second":32258.064516129034,"slot_id":0,"t_token":0.031,"t_token_generation":0.031,"task_id":0,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     290.71 ms","slot_id":0,"t_prompt_processing":290.681,"t_token_generation":0.031,"t_total":290.712,"task_id":0,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080165,"truncated":false}
[GIN] 2024/04/02 - 17:49:25 | 200 |  2.758009401s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.74 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.738,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":4,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.75 ms","slot_id":0,"t_prompt_processing":13.738,"t_token_generation":0.016,"t_total":13.754,"task_id":4,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080165,"truncated":false}
[GIN] 2024/04/02 - 17:49:25 | 200 |   25.230254ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.94 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.939,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":8,"tid":"139923868976704","timestamp":1712080165}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.96 ms","slot_id":0,"t_prompt_processing":12.939,"t_token_generation":0.017,"t_total":12.956,"task_id":8,"tid":"139923868976704","timestamp":1712080165}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080165,"truncated":false}
[GIN] 2024/04/02 - 17:49:25 | 200 |   21.563305ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:49:25.388Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:49:26.865Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:26.865Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:26.865Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:26.865Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:26.865Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:26.866Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:49:26.866Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139924531574336","timestamp":1712080168}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139924531574336","timestamp":1712080168}
time=2024-04-02T17:49:28.621Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080168}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080168}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080168}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080168}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     306.29 ms /   484 tokens (    0.63 ms per token,  1580.20 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1580.1966104129733,"slot_id":0,"t_prompt_processing":306.291,"t_token":0.6328326446280992,"task_id":0,"tid":"139923868976704","timestamp":1712080170}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1842.85 ms /   159 runs   (   11.59 ms per token,    86.28 tokens per second)","n_decoded":159,"n_tokens_second":86.27949782076438,"slot_id":0,"t_token":11.590238993710692,"t_token_generation":1842.848,"task_id":0,"tid":"139923868976704","timestamp":1712080170}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2149.14 ms","slot_id":0,"t_prompt_processing":306.291,"t_token_generation":1842.848,"t_total":2149.139,"task_id":0,"tid":"139923868976704","timestamp":1712080170}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":643,"n_ctx":2048,"n_past":642,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080170,"truncated":false}
[GIN] 2024/04/02 - 17:49:30 | 200 |   5.38788635s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:49:34.479Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:49:35.244Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:35.258Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:35.258Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:35.361Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:35.361Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:35.361Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:49:35.361Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139927660512832","timestamp":1712080176}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139927660512832","timestamp":1712080176}
time=2024-04-02T17:49:36.988Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080176}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080176}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080176}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080176}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     291.05 ms /   488 tokens (    0.60 ms per token,  1676.69 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1676.6878543205635,"slot_id":0,"t_prompt_processing":291.05,"t_token":0.5964139344262296,"task_id":0,"tid":"139923868976704","timestamp":1712080177}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":0,"tid":"139923868976704","timestamp":1712080177}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     291.07 ms","slot_id":0,"t_prompt_processing":291.05,"t_token_generation":0.016,"t_total":291.06600000000003,"task_id":0,"tid":"139923868976704","timestamp":1712080177}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080177,"truncated":false}
[GIN] 2024/04/02 - 17:49:37 | 200 |  2.804871455s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080177}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080177}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080177}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080177}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.69 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.686,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712080177}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      93.66 ms /    10 runs   (    9.37 ms per token,   106.77 tokens per second)","n_decoded":10,"n_tokens_second":106.76916506512919,"slot_id":0,"t_token":9.366,"t_token_generation":93.66,"task_id":4,"tid":"139923868976704","timestamp":1712080177}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     106.35 ms","slot_id":0,"t_prompt_processing":12.686,"t_token_generation":93.66,"t_total":106.346,"task_id":4,"tid":"139923868976704","timestamp":1712080177}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":498,"n_ctx":2048,"n_past":497,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080177,"truncated":false}
[GIN] 2024/04/02 - 17:49:37 | 200 |  113.658373ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712080181}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.54 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.535,"t_token":null,"task_id":17,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":17,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.55 ms","slot_id":0,"t_prompt_processing":14.535,"t_token_generation":0.016,"t_total":14.551,"task_id":17,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":17,"tid":"139923868976704","timestamp":1712080182,"truncated":false}
[GIN] 2024/04/02 - 17:49:42 | 200 |   23.686155ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.71 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.714,"t_token":null,"task_id":21,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":21,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.73 ms","slot_id":0,"t_prompt_processing":13.714,"t_token_generation":0.015,"t_total":13.729000000000001,"task_id":21,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":21,"tid":"139923868976704","timestamp":1712080182,"truncated":false}
[GIN] 2024/04/02 - 17:49:42 | 200 |   21.803864ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.58 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.579,"t_token":null,"task_id":25,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 52631.58 tokens per second)","n_decoded":1,"n_tokens_second":52631.57894736842,"slot_id":0,"t_token":0.019,"t_token_generation":0.019,"task_id":25,"tid":"139923868976704","timestamp":1712080182}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.60 ms","slot_id":0,"t_prompt_processing":14.579,"t_token_generation":0.019,"t_total":14.598,"task_id":25,"tid":"139923868976704","timestamp":1712080182}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":25,"tid":"139923868976704","timestamp":1712080182,"truncated":false}
[GIN] 2024/04/02 - 17:49:42 | 200 |    22.96089ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:49:42.128Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:49:43.514Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:43.514Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:43.514Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:43.514Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:43.514Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:43.514Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:49:43.514Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139926502893120","timestamp":1712080185}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139926502893120","timestamp":1712080185}
time=2024-04-02T17:49:45.325Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080185}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080185}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080185}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080185}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     307.30 ms /   484 tokens (    0.63 ms per token,  1575.02 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1575.0235114563434,"slot_id":0,"t_prompt_processing":307.297,"t_token":0.6349111570247934,"task_id":0,"tid":"139923868976704","timestamp":1712080187}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1441.68 ms /   131 runs   (   11.01 ms per token,    90.87 tokens per second)","n_decoded":131,"n_tokens_second":90.86595953065998,"slot_id":0,"t_token":11.005221374045801,"t_token_generation":1441.684,"task_id":0,"tid":"139923868976704","timestamp":1712080187}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1748.98 ms","slot_id":0,"t_prompt_processing":307.297,"t_token_generation":1441.684,"t_total":1748.981,"task_id":0,"tid":"139923868976704","timestamp":1712080187}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":615,"n_ctx":2048,"n_past":614,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080187,"truncated":false}
[GIN] 2024/04/02 - 17:49:47 | 200 |  4.952663069s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:49:51.540Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:49:52.254Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:52.255Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:52.255Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:52.255Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:49:52.255Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:49:52.255Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:49:52.255Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928625215040","timestamp":1712080193}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928625215040","timestamp":1712080193}
time=2024-04-02T17:49:53.890Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080193}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080193}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080193}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080193}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     292.18 ms /   488 tokens (    0.60 ms per token,  1670.20 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1670.2032993360256,"slot_id":0,"t_prompt_processing":292.18,"t_token":0.5987295081967213,"task_id":0,"tid":"139923868976704","timestamp":1712080194}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 35714.29 tokens per second)","n_decoded":1,"n_tokens_second":35714.28571428571,"slot_id":0,"t_token":0.028,"t_token_generation":0.028,"task_id":0,"tid":"139923868976704","timestamp":1712080194}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     292.21 ms","slot_id":0,"t_prompt_processing":292.18,"t_token_generation":0.028,"t_total":292.208,"task_id":0,"tid":"139923868976704","timestamp":1712080194}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080194,"truncated":false}
[GIN] 2024/04/02 - 17:49:54 | 200 |  2.646695084s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080194}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080194}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080194}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080194}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.33 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.328,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712080194}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      10.60 ms /     2 runs   (    5.30 ms per token,   188.70 tokens per second)","n_decoded":2,"n_tokens_second":188.69704689121616,"slot_id":0,"t_token":5.2995,"t_token_generation":10.599,"task_id":4,"tid":"139923868976704","timestamp":1712080194}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      23.93 ms","slot_id":0,"t_prompt_processing":13.328,"t_token_generation":10.599,"t_total":23.927,"task_id":4,"tid":"139923868976704","timestamp":1712080194}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":490,"n_ctx":2048,"n_past":489,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080194,"truncated":false}
[GIN] 2024/04/02 - 17:49:54 | 200 |   30.716239ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080198}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080198}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080198}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080198}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.78 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.776,"t_token":null,"task_id":9,"tid":"139923868976704","timestamp":1712080198}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      10.94 ms /     2 runs   (    5.47 ms per token,   182.80 tokens per second)","n_decoded":2,"n_tokens_second":182.79864729001005,"slot_id":0,"t_token":5.4705,"t_token_generation":10.941,"task_id":9,"tid":"139923868976704","timestamp":1712080198}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      24.72 ms","slot_id":0,"t_prompt_processing":13.776,"t_token_generation":10.941,"t_total":24.717,"task_id":9,"tid":"139923868976704","timestamp":1712080198}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":490,"n_ctx":2048,"n_past":489,"n_system_tokens":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080198,"truncated":false}
[GIN] 2024/04/02 - 17:49:58 | 200 |   34.127838ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.52 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.518,"t_token":null,"task_id":14,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":14,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.54 ms","slot_id":0,"t_prompt_processing":14.518,"t_token_generation":0.017,"t_total":14.535,"task_id":14,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":14,"tid":"139923868976704","timestamp":1712080202,"truncated":false}
[GIN] 2024/04/02 - 17:50:02 | 200 |   23.614393ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.74 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.744,"t_token":null,"task_id":18,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":18,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.76 ms","slot_id":0,"t_prompt_processing":14.744,"t_token_generation":0.017,"t_total":14.761,"task_id":18,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":18,"tid":"139923868976704","timestamp":1712080202,"truncated":false}
[GIN] 2024/04/02 - 17:50:02 | 200 |   23.333017ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      18.59 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":18.587,"t_token":null,"task_id":22,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 40000.00 tokens per second)","n_decoded":1,"n_tokens_second":40000.0,"slot_id":0,"t_token":0.025,"t_token_generation":0.025,"task_id":22,"tid":"139923868976704","timestamp":1712080202}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      18.61 ms","slot_id":0,"t_prompt_processing":18.587,"t_token_generation":0.025,"t_total":18.612,"task_id":22,"tid":"139923868976704","timestamp":1712080202}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":22,"tid":"139923868976704","timestamp":1712080202,"truncated":false}
[GIN] 2024/04/02 - 17:50:02 | 200 |   27.247894ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:50:02.954Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:50:04.393Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:04.393Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:04.393Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:04.393Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:04.393Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:04.394Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:50:04.394Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928625215040","timestamp":1712080206}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928625215040","timestamp":1712080206}
time=2024-04-02T17:50:06.152Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080206}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080206}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080206}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080206}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     306.60 ms /   484 tokens (    0.63 ms per token,  1578.62 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1578.6194907321337,"slot_id":0,"t_prompt_processing":306.597,"t_token":0.6334648760330578,"task_id":0,"tid":"139923868976704","timestamp":1712080208}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1973.78 ms /   178 runs   (   11.09 ms per token,    90.18 tokens per second)","n_decoded":178,"n_tokens_second":90.18224412941457,"slot_id":0,"t_token":11.088657303370786,"t_token_generation":1973.781,"task_id":0,"tid":"139923868976704","timestamp":1712080208}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2280.38 ms","slot_id":0,"t_prompt_processing":306.597,"t_token_generation":1973.781,"t_total":2280.3779999999997,"task_id":0,"tid":"139923868976704","timestamp":1712080208}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":662,"n_ctx":2048,"n_past":661,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080208,"truncated":false}
[GIN] 2024/04/02 - 17:50:08 | 200 |   5.48330076s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:50:12.145Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:50:12.865Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:12.865Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:12.865Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:12.865Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:12.865Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:12.866Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:50:12.866Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925336880704","timestamp":1712080214}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925336880704","timestamp":1712080214}
time=2024-04-02T17:50:14.527Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080214}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080214}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080214}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080214}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     293.61 ms /   488 tokens (    0.60 ms per token,  1662.09 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1662.0857132152842,"slot_id":0,"t_prompt_processing":293.607,"t_token":0.6016536885245902,"task_id":0,"tid":"139923868976704","timestamp":1712080214}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     175.31 ms /    17 runs   (   10.31 ms per token,    96.97 tokens per second)","n_decoded":17,"n_tokens_second":96.96831417733793,"slot_id":0,"t_token":10.31264705882353,"t_token_generation":175.315,"task_id":0,"tid":"139923868976704","timestamp":1712080214}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     468.92 ms","slot_id":0,"t_prompt_processing":293.607,"t_token_generation":175.315,"t_total":468.922,"task_id":0,"tid":"139923868976704","timestamp":1712080214}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":505,"n_ctx":2048,"n_past":504,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080214,"truncated":false}
[GIN] 2024/04/02 - 17:50:15 | 200 |  2.855587095s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712080219}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712080219}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712080219}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712080219}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.71 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.708,"t_token":null,"task_id":20,"tid":"139923868976704","timestamp":1712080219}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      54.74 ms /     6 runs   (    9.12 ms per token,   109.62 tokens per second)","n_decoded":6,"n_tokens_second":109.61707103186204,"slot_id":0,"t_token":9.122666666666666,"t_token_generation":54.736,"task_id":20,"tid":"139923868976704","timestamp":1712080219}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      68.44 ms","slot_id":0,"t_prompt_processing":13.708,"t_token_generation":54.736,"t_total":68.444,"task_id":20,"tid":"139923868976704","timestamp":1712080219}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":494,"n_ctx":2048,"n_past":493,"n_system_tokens":0,"slot_id":0,"task_id":20,"tid":"139923868976704","timestamp":1712080219,"truncated":false}
[GIN] 2024/04/02 - 17:50:19 | 200 |   77.064279ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:50:21.064Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:50:22.495Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:22.495Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:22.495Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:22.495Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:22.495Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:22.496Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:50:22.496Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925160699456","timestamp":1712080224}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925160699456","timestamp":1712080224}
time=2024-04-02T17:50:24.114Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080224}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080224}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":195,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080224}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080224}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     155.44 ms /   195 tokens (    0.80 ms per token,  1254.51 tokens per second)","n_prompt_tokens_processed":195,"n_tokens_second":1254.5114160538863,"slot_id":0,"t_prompt_processing":155.439,"t_token":0.7971230769230769,"task_id":0,"tid":"139923868976704","timestamp":1712080225}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1063.19 ms /    96 runs   (   11.07 ms per token,    90.29 tokens per second)","n_decoded":96,"n_tokens_second":90.29413313869932,"slot_id":0,"t_token":11.074916666666667,"t_token_generation":1063.192,"task_id":0,"tid":"139923868976704","timestamp":1712080225}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1218.63 ms","slot_id":0,"t_prompt_processing":155.439,"t_token_generation":1063.192,"t_total":1218.631,"task_id":0,"tid":"139923868976704","timestamp":1712080225}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":291,"n_ctx":2048,"n_past":290,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080225,"truncated":false}
[GIN] 2024/04/02 - 17:50:25 | 200 |  4.273456644s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:50:35.438Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:50:36.104Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:36.158Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:36.158Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:36.159Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:36.159Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:36.159Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:50:36.159Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925303309888","timestamp":1712080237}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925303309888","timestamp":1712080237}
time=2024-04-02T17:50:37.835Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080237}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080237}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080237}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080237}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     291.70 ms /   488 tokens (    0.60 ms per token,  1672.97 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1672.9746036969996,"slot_id":0,"t_prompt_processing":291.696,"t_token":0.5977377049180328,"task_id":0,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":0,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     291.71 ms","slot_id":0,"t_prompt_processing":291.696,"t_token_generation":0.017,"t_total":291.713,"task_id":0,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080238,"truncated":false}
[GIN] 2024/04/02 - 17:50:38 | 200 |  2.693709741s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.14 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.137,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":4,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.15 ms","slot_id":0,"t_prompt_processing":13.137,"t_token_generation":0.014,"t_total":13.151,"task_id":4,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080238,"truncated":false}
[GIN] 2024/04/02 - 17:50:38 | 200 |   22.058175ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.47 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.474,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 90909.09 tokens per second)","n_decoded":1,"n_tokens_second":90909.09090909091,"slot_id":0,"t_token":0.011,"t_token_generation":0.011,"task_id":8,"tid":"139923868976704","timestamp":1712080238}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.48 ms","slot_id":0,"t_prompt_processing":12.474,"t_token_generation":0.011,"t_total":12.485,"task_id":8,"tid":"139923868976704","timestamp":1712080238}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080238,"truncated":false}
[GIN] 2024/04/02 - 17:50:38 | 200 |    21.20051ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:50:38.204Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:50:39.622Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:39.622Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:39.622Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:39.623Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:39.623Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:39.623Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:50:39.623Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925865358912","timestamp":1712080241}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925865358912","timestamp":1712080241}
time=2024-04-02T17:50:41.507Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080241}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080241}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080241}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080241}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     308.76 ms /   484 tokens (    0.64 ms per token,  1567.54 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1567.5351804770619,"slot_id":0,"t_prompt_processing":308.765,"t_token":0.637944214876033,"task_id":0,"tid":"139923868976704","timestamp":1712080242}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     888.15 ms /    79 runs   (   11.24 ms per token,    88.95 tokens per second)","n_decoded":79,"n_tokens_second":88.94883865468823,"slot_id":0,"t_token":11.242417721518986,"t_token_generation":888.151,"task_id":0,"tid":"139923868976704","timestamp":1712080242}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1196.92 ms","slot_id":0,"t_prompt_processing":308.765,"t_token_generation":888.151,"t_total":1196.916,"task_id":0,"tid":"139923868976704","timestamp":1712080242}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":563,"n_ctx":2048,"n_past":562,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080242,"truncated":false}
[GIN] 2024/04/02 - 17:50:42 | 200 |  4.504442669s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:50:46.477Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:50:47.093Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:47.093Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:47.093Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:47.093Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:47.093Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:47.093Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:50:47.093Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928625215040","timestamp":1712080248}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928625215040","timestamp":1712080248}
time=2024-04-02T17:50:48.715Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080248}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080248}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080248}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080248}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     292.53 ms /   488 tokens (    0.60 ms per token,  1668.19 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1668.1935651484282,"slot_id":0,"t_prompt_processing":292.532,"t_token":0.5994508196721311,"task_id":0,"tid":"139923868976704","timestamp":1712080249}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      11.45 ms /     2 runs   (    5.72 ms per token,   174.67 tokens per second)","n_decoded":2,"n_tokens_second":174.67248908296943,"slot_id":0,"t_token":5.725,"t_token_generation":11.45,"task_id":0,"tid":"139923868976704","timestamp":1712080249}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     303.98 ms","slot_id":0,"t_prompt_processing":292.532,"t_token_generation":11.45,"t_total":303.98199999999997,"task_id":0,"tid":"139923868976704","timestamp":1712080249}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":490,"n_ctx":2048,"n_past":489,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080249,"truncated":false}
[GIN] 2024/04/02 - 17:50:49 | 200 |  2.546053975s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.50 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.502,"t_token":null,"task_id":5,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 76923.08 tokens per second)","n_decoded":1,"n_tokens_second":76923.07692307692,"slot_id":0,"t_token":0.013,"t_token_generation":0.013,"task_id":5,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.52 ms","slot_id":0,"t_prompt_processing":13.502,"t_token_generation":0.013,"t_total":13.515,"task_id":5,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":5,"tid":"139923868976704","timestamp":1712080252,"truncated":false}
[GIN] 2024/04/02 - 17:50:52 | 200 |   19.665767ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.47 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.473,"t_token":null,"task_id":9,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 71428.57 tokens per second)","n_decoded":1,"n_tokens_second":71428.57142857142,"slot_id":0,"t_token":0.014,"t_token_generation":0.014,"task_id":9,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.49 ms","slot_id":0,"t_prompt_processing":13.473,"t_token_generation":0.014,"t_total":13.487,"task_id":9,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080252,"truncated":false}
[GIN] 2024/04/02 - 17:50:52 | 200 |    22.11937ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.36 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.364,"t_token":null,"task_id":13,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":13,"tid":"139923868976704","timestamp":1712080252}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.38 ms","slot_id":0,"t_prompt_processing":14.364,"t_token_generation":0.017,"t_total":14.381,"task_id":13,"tid":"139923868976704","timestamp":1712080252}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":13,"tid":"139923868976704","timestamp":1712080252,"truncated":false}
[GIN] 2024/04/02 - 17:50:52 | 200 |   25.040437ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:50:52.874Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:50:54.245Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:54.245Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:54.245Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:54.245Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:50:54.245Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:50:54.245Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:50:54.246Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139926502893120","timestamp":1712080255}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139926502893120","timestamp":1712080255}
time=2024-04-02T17:50:55.989Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080255}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080255}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080255}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080255}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     309.01 ms /   484 tokens (    0.64 ms per token,  1566.28 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1566.2771469161491,"slot_id":0,"t_prompt_processing":309.013,"t_token":0.6384566115702479,"task_id":0,"tid":"139923868976704","timestamp":1712080258}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1750.41 ms /   160 runs   (   10.94 ms per token,    91.41 tokens per second)","n_decoded":160,"n_tokens_second":91.40705159699544,"slot_id":0,"t_token":10.940075,"t_token_generation":1750.412,"task_id":0,"tid":"139923868976704","timestamp":1712080258}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2059.43 ms","slot_id":0,"t_prompt_processing":309.013,"t_token_generation":1750.412,"t_total":2059.425,"task_id":0,"tid":"139923868976704","timestamp":1712080258}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":644,"n_ctx":2048,"n_past":643,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080258,"truncated":false}
[GIN] 2024/04/02 - 17:50:58 | 200 |  5.181229532s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:51:01.767Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:51:02.506Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:02.506Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:02.506Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:02.506Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:02.506Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:02.506Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:51:02.506Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925294917184","timestamp":1712080264}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925294917184","timestamp":1712080264}
time=2024-04-02T17:51:04.118Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080264}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080264}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080264}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080264}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     292.45 ms /   488 tokens (    0.60 ms per token,  1668.64 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1668.6384867363754,"slot_id":0,"t_prompt_processing":292.454,"t_token":0.5992909836065574,"task_id":0,"tid":"139923868976704","timestamp":1712080264}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     117.92 ms /    12 runs   (    9.83 ms per token,   101.77 tokens per second)","n_decoded":12,"n_tokens_second":101.7682228724081,"slot_id":0,"t_token":9.82625,"t_token_generation":117.915,"task_id":0,"tid":"139923868976704","timestamp":1712080264}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     410.37 ms","slot_id":0,"t_prompt_processing":292.454,"t_token_generation":117.915,"t_total":410.369,"task_id":0,"tid":"139923868976704","timestamp":1712080264}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":500,"n_ctx":2048,"n_past":499,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080264,"truncated":false}
[GIN] 2024/04/02 - 17:51:04 | 200 |  2.766688347s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.29 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.293,"t_token":null,"task_id":15,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 52631.58 tokens per second)","n_decoded":1,"n_tokens_second":52631.57894736842,"slot_id":0,"t_token":0.019,"t_token_generation":0.019,"task_id":15,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.31 ms","slot_id":0,"t_prompt_processing":14.293,"t_token_generation":0.019,"t_total":14.312,"task_id":15,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":15,"tid":"139923868976704","timestamp":1712080268,"truncated":false}
[GIN] 2024/04/02 - 17:51:08 | 200 |   24.319894ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.15 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.153,"t_token":null,"task_id":19,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":19,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.17 ms","slot_id":0,"t_prompt_processing":14.153,"t_token_generation":0.016,"t_total":14.169,"task_id":19,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":19,"tid":"139923868976704","timestamp":1712080268,"truncated":false}
[GIN] 2024/04/02 - 17:51:08 | 200 |    22.68696ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":488,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":487,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.80 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.796,"t_token":null,"task_id":23,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":23,"tid":"139923868976704","timestamp":1712080268}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.81 ms","slot_id":0,"t_prompt_processing":14.796,"t_token_generation":0.017,"t_total":14.812999999999999,"task_id":23,"tid":"139923868976704","timestamp":1712080268}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":23,"tid":"139923868976704","timestamp":1712080268,"truncated":false}
[GIN] 2024/04/02 - 17:51:08 | 200 |   24.111045ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:51:08.371Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:51:09.830Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:09.830Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:09.830Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:09.830Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:09.830Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:09.830Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:51:09.830Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139928735331904","timestamp":1712080271}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139928735331904","timestamp":1712080271}
time=2024-04-02T17:51:11.600Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080271}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080271}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":484,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080271}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080271}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     309.35 ms /   484 tokens (    0.64 ms per token,  1564.56 tokens per second)","n_prompt_tokens_processed":484,"n_tokens_second":1564.5607592645274,"slot_id":0,"t_prompt_processing":309.352,"t_token":0.6391570247933884,"task_id":0,"tid":"139923868976704","timestamp":1712080273}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1458.38 ms /   133 runs   (   10.97 ms per token,    91.20 tokens per second)","n_decoded":133,"n_tokens_second":91.19739436016114,"slot_id":0,"t_token":10.965225563909774,"t_token_generation":1458.375,"task_id":0,"tid":"139923868976704","timestamp":1712080273}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1767.73 ms","slot_id":0,"t_prompt_processing":309.352,"t_token_generation":1458.375,"t_total":1767.7269999999999,"task_id":0,"tid":"139923868976704","timestamp":1712080273}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":617,"n_ctx":2048,"n_past":616,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080273,"truncated":false}
[GIN] 2024/04/02 - 17:51:13 | 200 |  5.001081988s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:51:17.084Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:51:17.786Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:17.786Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:17.786Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:17.786Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:17.786Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:17.786Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:51:17.787Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139926494500416","timestamp":1712080279}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139926494500416","timestamp":1712080279}
time=2024-04-02T17:51:19.444Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080279}
[GIN] 2024/04/02 - 17:51:19 | 200 |  2.364497469s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080279}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":488,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080279}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080279}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     292.85 ms /   488 tokens (    0.60 ms per token,  1666.40 tokens per second)","n_prompt_tokens_processed":488,"n_tokens_second":1666.399177727619,"slot_id":0,"t_prompt_processing":292.847,"t_token":0.6000963114754098,"task_id":0,"tid":"139923868976704","timestamp":1712080279}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 47619.05 tokens per second)","n_decoded":1,"n_tokens_second":47619.04761904762,"slot_id":0,"t_token":0.021,"t_token_generation":0.021,"task_id":0,"tid":"139923868976704","timestamp":1712080279}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     292.87 ms","slot_id":0,"t_prompt_processing":292.847,"t_token_generation":0.021,"t_total":292.868,"task_id":0,"tid":"139923868976704","timestamp":1712080279}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":489,"n_ctx":2048,"n_past":488,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080279,"truncated":false}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":265,"n_past_se":0,"n_prompt_tokens_processed":213,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":265,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     151.52 ms /   213 tokens (    0.71 ms per token,  1405.77 tokens per second)","n_prompt_tokens_processed":213,"n_tokens_second":1405.7735714568566,"slot_id":0,"t_prompt_processing":151.518,"t_token":0.7113521126760564,"task_id":4,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 37037.04 tokens per second)","n_decoded":1,"n_tokens_second":37037.03703703704,"slot_id":0,"t_token":0.027,"t_token_generation":0.027,"task_id":4,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     151.54 ms","slot_id":0,"t_prompt_processing":151.518,"t_token_generation":0.027,"t_total":151.545,"task_id":4,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":479,"n_ctx":2048,"n_past":478,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080282,"truncated":false}
[GIN] 2024/04/02 - 17:51:22 | 200 |  159.444245ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":478,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":477,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.09 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.093,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":8,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.11 ms","slot_id":0,"t_prompt_processing":13.093,"t_token_generation":0.016,"t_total":13.109,"task_id":8,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":479,"n_ctx":2048,"n_past":478,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080282,"truncated":false}
[GIN] 2024/04/02 - 17:51:22 | 200 |   16.965458ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":12,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":478,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":12,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":12,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":477,"slot_id":0,"task_id":12,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      12.92 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":12.917,"t_token":null,"task_id":12,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second)","n_decoded":1,"n_tokens_second":62500.0,"slot_id":0,"t_token":0.016,"t_token_generation":0.016,"task_id":12,"tid":"139923868976704","timestamp":1712080282}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      12.93 ms","slot_id":0,"t_prompt_processing":12.917,"t_token_generation":0.016,"t_total":12.933,"task_id":12,"tid":"139923868976704","timestamp":1712080282}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":479,"n_ctx":2048,"n_past":478,"n_system_tokens":0,"slot_id":0,"task_id":12,"tid":"139923868976704","timestamp":1712080282,"truncated":false}
[GIN] 2024/04/02 - 17:51:22 | 200 |   19.817545ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:51:22.873Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:51:24.157Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:24.157Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:24.158Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:24.158Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:51:24.158Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:51:24.158Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:51:24.158Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925303309888","timestamp":1712080285}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925303309888","timestamp":1712080285}
time=2024-04-02T17:51:25.914Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080285}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080285}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":476,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080285}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080285}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     307.82 ms /   476 tokens (    0.65 ms per token,  1546.34 tokens per second)","n_prompt_tokens_processed":476,"n_tokens_second":1546.338167264411,"slot_id":0,"t_prompt_processing":307.824,"t_token":0.6466890756302521,"task_id":0,"tid":"139923868976704","timestamp":1712080287}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1380.62 ms /   128 runs   (   10.79 ms per token,    92.71 tokens per second)","n_decoded":128,"n_tokens_second":92.7123057478008,"slot_id":0,"t_token":10.7860546875,"t_token_generation":1380.615,"task_id":0,"tid":"139923868976704","timestamp":1712080287}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1688.44 ms","slot_id":0,"t_prompt_processing":307.824,"t_token_generation":1380.615,"t_total":1688.439,"task_id":0,"tid":"139923868976704","timestamp":1712080287}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":604,"n_ctx":2048,"n_past":603,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080287,"truncated":false}
[GIN] 2024/04/02 - 17:51:27 | 200 |  4.733306303s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":131,"tid":"139923868976704","timestamp":1712080289}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":135,"slot_id":0,"task_id":131,"tid":"139923868976704","timestamp":1712080289}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":131,"tid":"139923868976704","timestamp":1712080289}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     141.81 ms /   135 tokens (    1.05 ms per token,   951.98 tokens per second)","n_prompt_tokens_processed":135,"n_tokens_second":951.9847118307019,"slot_id":0,"t_prompt_processing":141.809,"t_token":1.050437037037037,"task_id":131,"tid":"139923868976704","timestamp":1712080292}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    2406.27 ms /   228 runs   (   10.55 ms per token,    94.75 tokens per second)","n_decoded":228,"n_tokens_second":94.75234106853212,"slot_id":0,"t_token":10.553828947368421,"t_token_generation":2406.273,"task_id":131,"tid":"139923868976704","timestamp":1712080292}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    2548.08 ms","slot_id":0,"t_prompt_processing":141.809,"t_token_generation":2406.273,"t_total":2548.0820000000003,"task_id":131,"tid":"139923868976704","timestamp":1712080292}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":373,"n_ctx":2048,"n_past":372,"n_system_tokens":0,"slot_id":0,"task_id":131,"tid":"139923868976704","timestamp":1712080292,"truncated":false}
[GIN] 2024/04/02 - 17:51:32 | 200 |  2.557580684s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:56:58.196Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:56:58.196Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:56:58.197Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:56:58.197Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:56:58.197Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:56:58.197Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:56:58.197Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925840180800","timestamp":1712080619}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925840180800","timestamp":1712080619}
time=2024-04-02T17:56:59.884Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080619}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080619}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":485,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080619}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080619}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     289.69 ms /   485 tokens (    0.60 ms per token,  1674.17 tokens per second)","n_prompt_tokens_processed":485,"n_tokens_second":1674.1745629023628,"slot_id":0,"t_prompt_processing":289.695,"t_token":0.5973092783505155,"task_id":0,"tid":"139923868976704","timestamp":1712080620}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 33333.33 tokens per second)","n_decoded":1,"n_tokens_second":33333.333333333336,"slot_id":0,"t_token":0.03,"t_token_generation":0.03,"task_id":0,"tid":"139923868976704","timestamp":1712080620}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     289.72 ms","slot_id":0,"t_prompt_processing":289.695,"t_token_generation":0.03,"t_total":289.72499999999997,"task_id":0,"tid":"139923868976704","timestamp":1712080620}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":486,"n_ctx":2048,"n_past":485,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080620,"truncated":false}
[GIN] 2024/04/02 - 17:57:00 | 200 |  2.638463057s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080620}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":485,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080620}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080620}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":484,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080620}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.26 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.256,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712080620}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =      10.96 ms /     2 runs   (    5.48 ms per token,   182.48 tokens per second)","n_decoded":2,"n_tokens_second":182.4817518248175,"slot_id":0,"t_token":5.48,"t_token_generation":10.96,"task_id":4,"tid":"139923868976704","timestamp":1712080620}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      24.22 ms","slot_id":0,"t_prompt_processing":13.256,"t_token_generation":10.96,"t_total":24.216,"task_id":4,"tid":"139923868976704","timestamp":1712080620}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":487,"n_ctx":2048,"n_past":486,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080620,"truncated":false}
[GIN] 2024/04/02 - 17:57:00 | 200 |   35.884145ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080624}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":485,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080624}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080624}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":484,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080624}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      15.33 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":15.334,"t_token":null,"task_id":9,"tid":"139923868976704","timestamp":1712080624}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     130.68 ms /    11 runs   (   11.88 ms per token,    84.17 tokens per second)","n_decoded":11,"n_tokens_second":84.17250772856663,"slot_id":0,"t_token":11.880363636363636,"t_token_generation":130.684,"task_id":9,"tid":"139923868976704","timestamp":1712080624}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     146.02 ms","slot_id":0,"t_prompt_processing":15.334,"t_token_generation":130.684,"t_total":146.018,"task_id":9,"tid":"139923868976704","timestamp":1712080624}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":496,"n_ctx":2048,"n_past":495,"n_system_tokens":0,"slot_id":0,"task_id":9,"tid":"139923868976704","timestamp":1712080624,"truncated":false}
[GIN] 2024/04/02 - 17:57:04 | 200 |   154.44193ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:57:06.030Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:57:07.358Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:57:07.358Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:57:07.358Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:57:07.358Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:57:07.358Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:57:07.358Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:57:07.358Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925336880704","timestamp":1712080629}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925336880704","timestamp":1712080629}
time=2024-04-02T17:57:09.150Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080629}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080629}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":108,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080629}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080629}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     118.04 ms /   108 tokens (    1.09 ms per token,   914.96 tokens per second)","n_prompt_tokens_processed":108,"n_tokens_second":914.9595892848065,"slot_id":0,"t_prompt_processing":118.038,"t_token":1.0929444444444445,"task_id":0,"tid":"139923868976704","timestamp":1712080629}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     249.56 ms /    24 runs   (   10.40 ms per token,    96.17 tokens per second)","n_decoded":24,"n_tokens_second":96.16771649757176,"slot_id":0,"t_token":10.3985,"t_token_generation":249.564,"task_id":0,"tid":"139923868976704","timestamp":1712080629}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     367.60 ms","slot_id":0,"t_prompt_processing":118.038,"t_token_generation":249.564,"t_total":367.602,"task_id":0,"tid":"139923868976704","timestamp":1712080629}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":132,"n_ctx":2048,"n_past":131,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080629,"truncated":false}
[GIN] 2024/04/02 - 17:57:09 | 200 |  3.491043611s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:59:09.784Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:59:10.511Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:59:10.511Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:59:10.511Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:59:10.512Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:59:10.512Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:59:10.512Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:59:10.512Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-fc59fa5607aaa5b5cfaa039c5a704d121d0f0cd353e2b8fe95cf08656838cfea (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139926502893120","timestamp":1712080752}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139926502893120","timestamp":1712080752}
time=2024-04-02T17:59:12.166Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080752}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":483,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     288.34 ms /   483 tokens (    0.60 ms per token,  1675.10 tokens per second)","n_prompt_tokens_processed":483,"n_tokens_second":1675.0999684401454,"slot_id":0,"t_prompt_processing":288.341,"t_token":0.5969792960662526,"task_id":0,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.03 ms /     1 runs   (    0.03 ms per token, 33333.33 tokens per second)","n_decoded":1,"n_tokens_second":33333.333333333336,"slot_id":0,"t_token":0.03,"t_token_generation":0.03,"task_id":0,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     288.37 ms","slot_id":0,"t_prompt_processing":288.341,"t_token_generation":0.03,"t_total":288.371,"task_id":0,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":484,"n_ctx":2048,"n_past":483,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080752,"truncated":false}
[GIN] 2024/04/02 - 17:59:12 | 200 |  2.674676397s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":483,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":482,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      13.11 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":13.111,"t_token":null,"task_id":4,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.01 ms /     1 runs   (    0.01 ms per token, 66666.67 tokens per second)","n_decoded":1,"n_tokens_second":66666.66666666667,"slot_id":0,"t_token":0.015,"t_token_generation":0.015,"task_id":4,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      13.13 ms","slot_id":0,"t_prompt_processing":13.111,"t_token_generation":0.015,"t_total":13.126000000000001,"task_id":4,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":484,"n_ctx":2048,"n_past":483,"n_system_tokens":0,"slot_id":0,"task_id":4,"tid":"139923868976704","timestamp":1712080752,"truncated":false}
[GIN] 2024/04/02 - 17:59:12 | 200 |   19.089228ms |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":483,"n_past_se":0,"n_prompt_tokens_processed":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1818,"msg":"we have to evaluate at least 1 token to generate logits","slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":482,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =      14.73 ms /     0 tokens (     inf ms per token,     0.00 tokens per second)","n_prompt_tokens_processed":0,"n_tokens_second":0.0,"slot_id":0,"t_prompt_processing":14.734,"t_token":null,"task_id":8,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 58823.53 tokens per second)","n_decoded":1,"n_tokens_second":58823.5294117647,"slot_id":0,"t_token":0.017,"t_token_generation":0.017,"task_id":8,"tid":"139923868976704","timestamp":1712080752}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =      14.75 ms","slot_id":0,"t_prompt_processing":14.734,"t_token_generation":0.017,"t_total":14.751,"task_id":8,"tid":"139923868976704","timestamp":1712080752}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":484,"n_ctx":2048,"n_past":483,"n_system_tokens":0,"slot_id":0,"task_id":8,"tid":"139923868976704","timestamp":1712080752,"truncated":false}
[GIN] 2024/04/02 - 17:59:12 | 200 |   18.753483ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2024-04-02T17:59:12.527Z level=INFO source=routes.go:79 msg="changing loaded model"
time=2024-04-02T17:59:13.896Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:59:13.896Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:59:13.896Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:59:13.896Z level=INFO source=gpu.go:188 msg="[cudart] CUDART CUDA Compute Capability detected: 8.6"
time=2024-04-02T17:59:13.897Z level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
time=2024-04-02T17:59:13.897Z level=INFO source=dyn_ext_server.go:87 msg="Loading Dynamic llm server: /tmp/ollama1664137075/runners/cuda_v11/libext_server.so"
time=2024-04-02T17:59:13.897Z level=INFO source=dyn_ext_server.go:147 msg="Initializing llama server"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = mistralai
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = ["▁ t", "i n", "e r", "▁ a", "h e...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = mistralai
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    70.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
loading library /tmp/ollama1664137075/runners/cuda_v11/libext_server.so
{"function":"initialize","level":"INFO","line":444,"msg":"initializing slots","n_slots":1,"tid":"139925831788096","timestamp":1712080755}
{"function":"initialize","level":"INFO","line":453,"msg":"new slot","n_ctx_slot":2048,"slot_id":0,"tid":"139925831788096","timestamp":1712080755}
time=2024-04-02T17:59:15.738Z level=INFO source=dyn_ext_server.go:159 msg="Starting llama main loop"
{"function":"update_slots","level":"INFO","line":1572,"msg":"all slots are idle and system prompt is empty, clear the KV cache","tid":"139923868976704","timestamp":1712080755}
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080755}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":0,"n_past_se":0,"n_prompt_tokens_processed":479,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080755}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080755}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     305.68 ms /   479 tokens (    0.64 ms per token,  1566.99 tokens per second)","n_prompt_tokens_processed":479,"n_tokens_second":1566.987915546221,"slot_id":0,"t_prompt_processing":305.682,"t_token":0.6381670146137788,"task_id":0,"tid":"139923868976704","timestamp":1712080756}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =     651.10 ms /    61 runs   (   10.67 ms per token,    93.69 tokens per second)","n_decoded":61,"n_tokens_second":93.68818115915319,"slot_id":0,"t_token":10.673704918032787,"t_token_generation":651.096,"task_id":0,"tid":"139923868976704","timestamp":1712080756}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =     956.78 ms","slot_id":0,"t_prompt_processing":305.682,"t_token_generation":651.096,"t_total":956.778,"task_id":0,"tid":"139923868976704","timestamp":1712080756}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":540,"n_ctx":2048,"n_past":539,"n_system_tokens":0,"slot_id":0,"task_id":0,"tid":"139923868976704","timestamp":1712080756,"truncated":false}
[GIN] 2024/04/02 - 17:59:16 | 200 |  4.172542335s |       127.0.0.1 | POST     "/v1/chat/completions"
{"function":"launch_slot_with_data","level":"INFO","line":826,"msg":"slot is processing task","slot_id":0,"task_id":64,"tid":"139923868976704","timestamp":1712080758}
{"function":"update_slots","ga_i":0,"level":"INFO","line":1803,"msg":"slot progression","n_past":10,"n_past_se":0,"n_prompt_tokens_processed":174,"slot_id":0,"task_id":64,"tid":"139923868976704","timestamp":1712080758}
{"function":"update_slots","level":"INFO","line":1830,"msg":"kv cache rm [p0, end)","p0":10,"slot_id":0,"task_id":64,"tid":"139923868976704","timestamp":1712080758}
{"function":"print_timings","level":"INFO","line":265,"msg":"prompt eval time     =     145.58 ms /   174 tokens (    0.84 ms per token,  1195.22 tokens per second)","n_prompt_tokens_processed":174,"n_tokens_second":1195.219123505976,"slot_id":0,"t_prompt_processing":145.58,"t_token":0.8366666666666668,"task_id":64,"tid":"139923868976704","timestamp":1712080760}
{"function":"print_timings","level":"INFO","line":279,"msg":"generation eval time =    1355.97 ms /   127 runs   (   10.68 ms per token,    93.66 tokens per second)","n_decoded":127,"n_tokens_second":93.6595438706466,"slot_id":0,"t_token":10.676968503937006,"t_token_generation":1355.975,"task_id":64,"tid":"139923868976704","timestamp":1712080760}
{"function":"print_timings","level":"INFO","line":289,"msg":"          total time =    1501.55 ms","slot_id":0,"t_prompt_processing":145.58,"t_token_generation":1355.975,"t_total":1501.5549999999998,"task_id":64,"tid":"139923868976704","timestamp":1712080760}
{"function":"update_slots","level":"INFO","line":1634,"msg":"slot released","n_cache_tokens":311,"n_ctx":2048,"n_past":310,"n_system_tokens":0,"slot_id":0,"task_id":64,"tid":"139923868976704","timestamp":1712080760,"truncated":false}
[GIN] 2024/04/02 - 17:59:20 | 200 |  1.507890024s |       127.0.0.1 | POST     "/v1/chat/completions"
